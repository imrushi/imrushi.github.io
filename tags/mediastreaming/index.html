<!doctype html><html lang=en><head><title>MediaStreaming :: Rushi Panchariya</title>
<meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content><meta name=keywords content><meta name=robots content="noodp"><link rel=canonical href=/tags/mediastreaming/><script async src="https://www.googletagmanager.com/gtag/js?id=G-P4ZXCNLZ0R"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-P4ZXCNLZ0R")}</script><link rel=stylesheet href=/css/buttons.min.2bc533403a27dfe0e93105a92502b42ce4587e2e4a87d9f7d349e51e16e09478.css><link rel=stylesheet href=/css/code.min.00125962708925857e7b66dbc58391d55be1191a3d0ce2034de8c9cd2c481c36.css><link rel=stylesheet href=/css/fonts.min.4881f0c525f3ce2a1864fb6e96676396cebe1e6fcef1933e8e1dde7041004fb5.css><link rel=stylesheet href=/css/footer.min.2e3eb191baee58dd05a9f0104ac1fab0827bca7c64dafe0b2579f934c33a1d69.css><link rel=stylesheet href=/css/gist.min.a751e8b0abe1ba8bc53ced52a38b19d8950fe78ca29454ea8c2595cf26aad5c0.css><link rel=stylesheet href=/css/header.min.b6fb4423cf82a9f9d7abc9cd010223fa3d70a6526a3f28f8e17d814c06e18f9e.css><link rel=stylesheet href=/css/main.min.fe8dc560fccb53a458b0db19ccb7b265764ac46b68596b7e099c6793054dd457.css><link rel=stylesheet href=/css/menu.min.83637a90d903026bc280d3f82f96ceb06c5fc72b7c1a8d686afb5bbf818a29f7.css><link rel=stylesheet href=/css/pagination.min.82f6400eae7c7c6dc3c866733c2ec0579e4089608fea69400ff85b3880aa0d3c.css><link rel=stylesheet href=/css/post.min.fc74ca360273c1d828da3c02b8174eba435607b369d98418ccc6f2243cd4e75d.css><link rel=stylesheet href=/css/prism.min.9023bbc24533d09e97a51a0a42a5a7bfe4c591ae167c5551fb1d2191d11977c0.css><link rel=stylesheet href=/css/syntax.min.cc789ed9377260d7949ea4c18781fc58959a89287210fe4edbff44ebfc1511b6.css><link rel=stylesheet href=/css/terminal.min.dd0bf9c7cacb24c1b0184f52f1869b274e06689557468cc7030ccf632328eb97.css><link rel=stylesheet href=/css/terms.min.b81791663c3790e738e571cdbf802312390d30e4b1d8dc9d814a5b5454d0ac11.css><link rel=stylesheet href=/terminal.css><link rel=stylesheet href=/style.css><link rel="shortcut icon" href=/favicon.png><link rel=apple-touch-icon href=/apple-touch-icon.png><meta name=twitter:card content="summary"><meta name=twitter:site content="RushiPanchariya"><meta name=twitter:creator content><meta property="og:locale" content="en"><meta property="og:type" content="website"><meta property="og:title" content="MediaStreaming"><meta property="og:description" content><meta property="og:url" content="/tags/mediastreaming/"><meta property="og:site_name" content="Rushi Panchariya"><meta property="og:image" content="/og-image.png"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="627"><link href=/tags/mediastreaming/index.xml rel=alternate type=application/rss+xml title="Rushi Panchariya"></head><body><div class="container center"><header class=header><div class=header__inner><div class=header__logo><a href=/><div class=logo>Rushi Panchariya</div></a></div><ul class="menu menu--mobile"><li class=menu__trigger>Menu&nbsp;‚ñæ</li><li><ul class=menu__dropdown><li><a href=/about>About</a></li><li><a href=/projects>Projects</a></li><li><a href=/notes>Notes</a></li><li><a href=/resume>Resume</a></li><li><a href=https://archive.ruship.dev>Archive site</a></li></ul></li></ul></div><nav class=navigation-menu><ul class="navigation-menu__inner menu--desktop"><li><a href=/about>About</a></li><li><a href=/projects>Projects</a></li><li><a href=/notes>Notes</a></li><li><a href=/resume>Resume</a></li><li><ul class=menu><li class=menu__trigger>Show more&nbsp;‚ñæ</li><li><ul class=menu__dropdown><li><a href=https://archive.ruship.dev>Archive site</a></li></ul></li></ul></li></ul></nav></header><div class=content><h1>Posts for: #MediaStreaming</h1><div class=posts><article class="post on-list"><h1 class=post-title><a href=/posts/rpi-home-server/>RPi Home Server</a></h1><div class=post-meta><time class=post-date>2024-05-16</time><span class=post-author>Rushi Panchariya</span></div><span class=post-tags>#<a href=/tags/rpi/>RPi</a>&nbsp;
#<a href=/tags/homeserversetup/>HomeServerSetup</a>&nbsp;
#<a href=/tags/mediastreaming/>MediaStreaming</a>&nbsp;
#<a href=/tags/torrentmanagement/>TorrentManagement</a>&nbsp;
#<a href=/tags/networksecurity/>NetworkSecurity</a>&nbsp;
#<a href=/tags/cloud/>cloud</a>&nbsp;
</span><img src="https://ik.imagekit.io/ruship/RPi-Home-Server/cover.jpg?tr=w-500,h-350" class=post-cover alt="RPi Home Server" title="Cover Image"><div class=post-content><p>Dive into the intricate setup of a Raspberry Pi Home Server, showcasing storage solutions, media streaming capabilities, torrent management, and secure network access. Discover how to optimize resources, manage services, and plan for future upgrades to enhance your home server experience.</p></div><div><a class="read-more button inline" href=/posts/rpi-home-server/>Read more</a></div></article><div class=pagination><div class=pagination__buttons></div></div></div></div><script>window.store={"/posts/kindle-ads-remover/":{title:"Ads on My Kindle? Not on My Watch!",tags:["Kindle","RPi","Bash"],categories:[],keywords:["Kindle ad removal","Kindle ads","block Kindle ads","Kindle hacks","remove ads from Kindle","Kindle AdRemover","Git Bash Kindle","Kindle system directory","Raspberry Pi Kindle connection","PowerShell script Kindle","Linux Kindle troubleshooting","Kindle update ads","ad-free Kindle","Kindle 12th gen ads","Kindle storage issue","manual ad removal Kindle","KindleAdRemover","DIY solutions","funny tech stories"],content:`Today, while exploring the settings on my Kindle. To my surprise, there was a software update available. I was connected to Wi-Fi (I usually keep it in Airplane mode). After updating, I noticed an annoying change ‚Äî my Kindle&rsquo;s lock screen was suddenly flooded with ads.
I hate Ads If you&rsquo;ve read my previous posts, you know I have AdGuard installed to block ads because no one loves ads. I&rsquo;ve already paid a lot for my Kindle, so why should I pay extra to remove ads?
When I purchased my Kindle in Germany, there were two versions available: one with ads and one without. The ad-free version cost an additional 10 Euros, but I decided not to spend the extra money ‚Äî I‚Äôd rather buy coffee or chocolates instead! At the time, I thought there might be a workaround to remove the ads, and since they didn‚Äôt show up initially, I ignored the issue.
However, after installing today‚Äôs Kindle update, ads started showing up on the lock screen. They were bright and intrusive, completely ruining the experience ‚òπÔ∏è.
I initially considered adding the ad servers‚Äô DNS to AdGuard but wanted a permanent solution.
Kindle Ad Remover After a bit of Googling, I found a GitHub repository called KindleAdRemover. According to the README, the process might require a Mac, which I have, but it&rsquo;s an office device, so I couldn‚Äôt use it for this purpose. Thankfully, the repo also provided a shell script, and I decided to give that a try.
The shell script was straightforward. It checked if the Kindle was connected and detected. Once detected, it navigated to the Kindle&rsquo;s system directory, deleted the .assets folder, and replaced it with an empty .assets file.
Following the instructions, I connected my Kindle to my Raspberry Pi.
Life won&rsquo;t be easy/fun without Issue - this the one thing I&rsquo;ve learned.
Issue: Connecting Kindle with Linux When I connected my Kindle to the Raspberry Pi, it appeared in lsusb, but the storage wasn‚Äôt mounting. I checked with fdisk -l, but no storage device was listed. Even dmesg | tail showed Kindle-related information but no mount path.
I suspect the issue lies with the latest Kindle (12th generation) update. The same problem occurred when I tried connecting it to a Mac (My Friends Macbook üòè).
I‚Äôll look into this issue further and try to fix it.
Mission: Kill the Ads Determined to get rid of the ads, I found a PowerShell script in the repository. I connected my Kindle to a Windows PC and saw the storage appear in File Explorer. Inside the system directory, I located the .assets folder.
Rather than deleting it manually, I tried running the PowerShell script, but it didn‚Äôt work. Instead, I used Git Bash to manually execute the required steps:
Navigated to the .assets directory inside the system folder. Opened Git Bash in the Kindle storage directory. Deleted the .assets folder using: $ rm -r system/.assets Created an empty .assets file using: $ touch system/.assets Disconnect Kindle. After locking and unlocking the Kindle, all the ads were gone, and a nice wallpaper appeared instead.
Note: These steps will need to be repeated after every software update, as updates revert the changes.
Now, my mind can fully enjoy reading without being distracted by ads! üòâ
`,url:"/posts/kindle-ads-remover/"},"/posts/":{title:"Posts",tags:[],categories:[],keywords:[],content:"",url:"/posts/"},"/posts/rpi-home-server/":{title:"RPi Home Server",tags:["RPi","HomeServerSetup","MediaStreaming","TorrentManagement","NetworkSecurity","cloud"],categories:[],keywords:["raspberry pi setup","raspberry pi home server","media streaming","network security","rpi docker applications","plex alternative","jellyfin","diy home server","self-hosted services","personal cloud storage","tailscale","wireguard vpn","immitch","open-source software","raspberry pi projects","casaos","adguard"],content:`I did a project on the Raspberry Pi in college. Since then, my Raspberry Pi has been lying in a box. I had the idea to create an FPV drone from it, which I am still planning to do üòÖ. Until I work on that, I&rsquo;m thinking of creating a mini server (which I was inspired to do after seeing a RPi stack server photo on Twitter/X).
I was planning to make something like the following:
Store documents, codes (unfinished or WIP), videos, and photos. Stream Movies and TV Shows Block ads, trackers, and malware over the network. Access all these over the internet Raspberry Pi 4 Model B Specs: 2GB RAM 128GB SD Card 15W USB-C Power Supply 1.8GHz Quad Core Processor I know the specs are low, but they get the job done for now. In the future, I will upgrade to RPi 5 and connect an SSD.
I am running Debian GNU/Linux 12 (Bookworm) OS along with Docker.
Application Setups: CasaOS: Website Source Category Similar Apps CasaOS GitHub cloud os - Casa OS is an open-source project that creates a personal cloud experience. It provides a web-based interface for managing your personal cloud and applications. It is set up and installed with a single command. Casa OS uses Docker containers to run applications. It has an app store with lots of apps that are easy to manage through the UI. It also provides CLI access to the RPi over the web. I really like its design, ease of setup, and installation of apps. Its dashboard offers monitoring options for CPU, Memory, Disk, Network, etc.
Demo
username - casaos
password - casaos
NextCloud Website Source Category Similar Apps Nextcloud GitHub storage - Nextcloud is an open-source cloud server. I am using it for storing my documents and files. My other online storage service is also getting full. This is the best alternative for that. It works well and gets the job done, but it lags sometimes (I think because of my low resources). I am thinking of installing NextcloudPi, which I think is a lightweight version of Nextcloud.
Immich Website Source Category Similar Apps Immich GitHub storage Google Photos Immich is a promising self-hosted alternative to Google Photos, particularly for users who value privacy and control over their data. It is an open-source app with client applications available for Android and iOS, allowing users to share photos with friends and family. I am using it for storing my photos and videos. Immich offers features similar to Google Photos, including photo organization, face recognition, mobile app, and sharing options. However, since Immich is under active development, some features might be less polished compared to Google Photos. Nonetheless, it works well and gets the job done.
Check out the comparison between Immich and various OpenSource Photo Libraries.
Syncthing Website Source Category Similar Apps Syncthing GitHub storage - Syncthing is an open-source file synchronization application that keeps our data consistent across multiple devices. It offers real-time updates, multi-device compatibility, and efficient file management.
I have been using Syncthing for syncing the projects which I am working on my PC. While I know there is GitHub to push the project, I prefer not to push buggy code or unfinished features. Syncthing makes it easy to access the code on the RPi from anywhere.
AdGuard Website Source Category Similar Apps AdGuard GitHub privacy Pi-Hole I have been using uBlock Origin in the browser for a long time, but it only securely blocks ads in my browser. I wanted a solution that blocks ads over my entire network. AdGuard is a free and open-source privacy blocker that blocks ads, trackers, and malware. There are many options to filter ads, trackers, and malware:
DNS Blocklist :- Below is Blocklist which i am using: AdGuard DNS filter :- https://adguardteam.github.io/HostlistsRegistry/assets/filter_1.txt AdAway Default Blocklist :- https://adguardteam.github.io/HostlistsRegistry/assets/filter_2.txt HaGeZi&rsquo;s Pro Blocklist :- https://adguardteam.github.io/HostlistsRegistry/assets/filter_48.txt OISD Blocklist Big :- https://adguardteam.github.io/HostlistsRegistry/assets/filter_27.txt Easylist :- https://easylist.to/easylist/easylist.txt uBlock Filter :- https://raw.githubusercontent.com/uBlockOrigin/uAssets/master/filters/filters.txt DNS Allowlist DNS Rewrite Blocked Services Custom Filterling Rules This list is easy to maintain if there are any updates to the block list. It can be updated with just one click.
I have configured my router to use the Raspberry Pi as the primary DNS server, allowing me to extend AdGuard&rsquo;s protection to all devices on my network. This way, every device benefits from an ad-free and secure browsing experience.
Now it&rsquo;s time for the best part, setting up Movies and TV Show streaming over the network. I&rsquo;m using Torrents to download movies and TV shows.
For this setup, a group of applications are deployed and interconnected. First, I&rsquo;ll provide information about each application, and then I&rsquo;ll explain how to set it up.
Jackett Website Source Category Similar Apps Jackett GitHub torrent - Jackett is an open-source application that acts as a middleman between your favorite torrent downloader and various torrent indexing websites. It allows you to search across a vast array of indexing sites from a single, user-friendly interface. Jackett seamlessly integrates with popular torrent downloaders like Sonarr, Radarr, qBittorrent, and many more, eliminating the need to configure individual search settings for each downloader.
Jackett acts as a proxy server. When anyone initiates a search through your compatible downloader, Jackett translates the request into a format understandable by the specific torrent indexing site. It then fetches the results, parses them, and delivers them back to your downloader in a standardized format.
I mostly use the following indexers for searching torrents:
YTS :- https://yts.mx/ TheRARBG :- https://therarbg.to/ Note: It&rsquo;s important to remember that Jackett itself is a legal tool. However, the legality of the content you download using Jackett depends on the copyright laws in your region and the specific content itself. Always ensure you&rsquo;re downloading content that is legally available in your area.
Radarr Website Source Category Similar Apps Radarr GitHub torrent Sonarr If you&rsquo;re a movie buff who relies on BitTorrent to build your collection, then Radarr is your new best friend. This open-source software automates the process of finding, downloading, and organizing your movie library and keeps your collection up-to-date. Tell Radarr which movies you want (and in what quality), and it scours the web using your Jackett indexers. Once it finds a match, Radarr automatically downloads the movie for you. You can add upcoming movies to Radarr&rsquo;s watchlist, and it will automatically download them on their release date or whenever they become available on your chosen indexers.
Plex Website Source Category Similar Apps Plex GitHub torrent - Plex is a media server application that lets you stream your own collection of movies, TV shows, music, and photos to various devices. It&rsquo;s like a personal streaming service, but with complete control over your content and how you access it. Plex centralizes your media collection, offers flexible streaming options, and provides a user-friendly interface for various devices. Add your movies, TV shows, music, and photos to a server (computer, NAS) running the Plex Media Server software. Then, access that content from any Plex app on various devices like smartphones, tablets, smart TVs, game consoles, and even streaming devices.
Overseerr Website Source Category Similar Apps Overseerr GitHub torrent - Overseerr is an open-source software application designed to simplify Movie and TV show requests within your Plex media server setup. It integrates seamlessly with Plex, Sonarr (for TV shows), and Radarr (for movies), creating a centralized hub for managing user requests. It provides a user-friendly interface for your Plex users to request movies and TV shows they&rsquo;d like to see added to your library. They can browse through suggestions, search for specific titles, and submit requests with a few clicks.
Transmission Website Source Category Similar Apps Transmission GitHub torrent - Transmission is an open-source BitTorrent client, known for its simplicity, speed, and cross-platform compatibility. It allows us to download files from the BitTorrent network, a decentralized peer-to-peer file-sharing system.
Working of Streaming Setup After reviewing the information about all the components I used, let&rsquo;s delve into their connectivity.
Movie or TV show requests will come to Radarr with parameters like quality, tags, etc. Radarr will search for the Movie/TV show using the Jackett torrent indexer and fetch torrent information. Once Radarr obtains the torrent information, it sends it to Transmission for downloading and monitors the torrent&rsquo;s download progress. After Transmission completes the download, it saves the content to a specific path. Radarr then moves the downloaded content to the Plex content directory. The content is now accessible on Plex for streaming. Note: Sometimes, certain TV shows will not be downloaded through Radarr; in such cases, use Sonarr.
To securely access my home network over the internet, I am using VPN.
Initially, I set up the WireGuard VPN with DDNS. However, due to my ISP&rsquo;s use of CGNAT, I was unable to access the internet.
Then, I learned about Tailscale from a friend.
Tailscale Website Source Category Similar Apps Tailscale GitHub VPN - Tailscale is a peer-to-peer VPN service designed to make your devices and applications accessible securely from anywhere in the world. It also utilizes WireGuard VPN in the backend. Tailscale employs a mesh network approach to connect multiple devices, allowing up to 100 devices in the free tier. It is available on all platforms, making device connectivity seamless. Additionally, Tailscale offers useful features such as inviting members to access your network and sharing specific devices.
As I continue to refine my Raspberry Pi Home Server setup, I&rsquo;m considering changing the streaming components by replacing:
Plex =&gt; Jellyfin Overseerr =&gt; Jellyseer With only 2 GB of RAM, running all these services simultaneously can be challenging. As a result, I&rsquo;ve opted not to run more resource-intensive services like Immich continuously. Instead, I activate this service when I need to periodically back up my data.
Looking ahead, I&rsquo;m contemplating upgrading my Raspberry Pi to a Raspberry Pi 5 with 8 GB of RAM and attaching an SSD to it. This upgrade would provide more power and storage capacity, allowing for smoother operation and greater flexibility in running multiple services concurrently.
This concludes my setup for the Raspberry Pi Home Server. Any changes or additional components will be detailed in future posts.
`,url:"/posts/rpi-home-server/"},"/posts/personal-access-token/":{title:"Personal Access Token (PAT)",tags:["PAT","Personal Access Token"],categories:[],keywords:["Personal Access Token","PAT","token generation","token management","token validation","HMAC","Hash-based Message Authentication Code","token expiry","OAuth2","Opaque Access Tokens","open source","third-party integration","security","go","generate PAT in go","PAT in Golang"],content:`I always had a question about how the Personal Access Token works and how it&rsquo;s generated and managed. In this blog, I will cover how to create your own PAT (Personal Access Token) system.
For years, logging in to things online relied on usernames and passwords. But sometimes passwords can be a pain! They&rsquo;re easy to forget and even harder to keep truly secure.
This is where Personal Access Tokens (PATs) come in. They&rsquo;re a new way to log in to apps and services, and they offer some big advantages over passwords.
Here&rsquo;s why passwords can be frustrating:
Leaky: Hackers can steal passwords in all sorts of ways, leaving your accounts vulnerable. Forgettable: Who remembers dozens of complex passwords? We all reuse passwords, which is a security risk. Inflexible: Passwords are all-or-nothing. You either have access or you don&rsquo;t. PATs aim to solve this problems by offering a more secure and flexible way to log in.
What are PATs? Personal Access Tokens (PATs) are long strings of random characters that act as digital credentials, enabling secure access to resources without the need for a password. These tokens are generated and managed by the user, granting them granular control over the scope of access granted to each PAT.
These tokens are stored in a database, and their validity is checked by performing a database lookup. But in this blog, I will also explain alternate approaches to checking the validity of a token. To revoke a token, you can simply delete it from the database. Once the token has been revoked, it can no longer be used.
PATs also called as Opaque Access Tokens (OATs) in OAuth 2.0.
You can check this ORY documentation to learn more about PATs.
example of a PAT:
org_at_QlANllvSYMuPiEw_f0K_GrKW05PGRc29w7V5HZ4434.N0_lECUd927GnG71eEKq1D7p8UagEj4xdf8ivvge5QhjJZcL99HBWvYjkEK8eUaheUfBFYN0tbIqcdafarsZXA In above PAT on is divided in two segments with dot operator (.) on left side of dot operator is 32-byte random string with base64 encoding and on right side of dot operator we have cryptographic signature with base64 encoding which generated from 32-byte random string.
Advantages and Characteristics PAT offers several distinct advantages over traditional passwords:
Enhanced Security: PATs can be granted specific permissions, limiting the potential damage if compromised. Flexibility: PATs can be tailored to specific tasks or applications, enabling precise control over access privileges. Third-party Integration: PAT facilitates secure integration with third-party tools and applications, eliminating the need to share passwords. PATs are characterized by several key features:
Unique Generation: Each PAT is a unique string of characters, preventing unauthorized access. Granular Permission: PATs can be granted specific permissions, limiting their scope of access. Revocability: PATs can be revoked at any time, effectively disabling them in cases of compromise. Working &amp; Implementation Let&rsquo;s first understand the how token is formed.
org_at_QlANllvSYMuPiEw_f0K_GrKW05PGRc29w7V5HZ4434.N0_lECUd927GnG71eEKq1D7p8UagEj4xdf8ivvge5QhjJZcL99HBWvYjkEK8eUaheUfBFYN0tbIqcdafarsZXA In the above PAT, it is divided into two segments with a dot operator (.). On the left side of the dot operator is a 32-byte random string with base64 encoding, and on the right side of the dot operator, we have a cryptographic signature with base64 encoding, which is generated from a 32-byte random string.
At the start, I have appended the org_at_ prefix to the token to identify the organization and token type like here _at_ is stands for Access Token. It is not mandatory; you can skip this.
Working Overview The app will request a PAT with scopes. PAT Generation will do below: Generate: This will generate a token with a random string and sign that token with a cryptographic algorithm, which is HMAC1. It will then store the token&rsquo;s name, signature, scope, granted scope against the user ID. Verify: Verification will decode the token and again generate the HMAC1 with the decoded token (left side of the dot) with the signing key. Then it will compare both HMACs1; if they are equal, the token is not tampered. If the token is not tampered, it should check in the database if a signature entry exists. Revoke: To revoke a token, we need to delete the token from the database. Database is used to store the token related information. As we have seen above, for validating/checking if the token is valid, we have to perform a database lookup. Let&rsquo;s examine the database schema for Personal Access Tokens.
PAT Table:
token_id user_id name signature scope granted_scope active created_at efaeb2c2-8654-4b83-a254-1ab1296614b2 6c1acbd1-602b-4235 token-name N0_lECUd927GnG71eEKq1&hellip; email name email 1 2024-01-24 12:09:22 sql user token table CREATE TABLE user_tokens ( token_id VARCHAR(36) NOT NULL, user_id VARCHAR(36) NOT NULL, name VARCHAR(255) NOT NULL, signature VARCHAR(255) NOT NULL, scope TEXT NOT NULL, granted_scope TEXT NOT NULL, active INTEGER NOT NULL DEFAULT true, created_at TIMESTAMP NOT NULL, PRIMARY KEY (token_id), UNIQUE KEY unique_token (signature) USING BTREE, KEY user_id_index (user_id) USING BTREE, KEY name_index (name) USING BTREE ); PAT Generation Generation of PAT using HMAC1:
We require 32-byte long secret. We can use one global secret or rotated secret. This secret will be used as signing key in HMAC1.
We can take input for how long token key should be if it is not defined default should be 32-byte.
Using above token entropy, we can generate random bytes of specified length.
By given token key and signature key we can generate HMAC1.
In HMAC, we can use sha512 or sha256. Using one them, create hash for token key (which is data) and hash key as signature key.
With above HMAC process, we get signature. We create base64 for signature and random bytes.
Concatenate above both base64 with dot operator as separator. Example token_base64.signature_base64.
When you are storing the token in the database, store its signature along with whatever data you want to store in the DB.
Implementation main.go:
Setting the token entropy to 32-byte for generating random bytes.
Creating context for managing cancellation signals and deadlines across API boundaries.
Generate function generates and encoded token and signature for provided context(request).
Validate function verifies provided token and signature.
Generate function:
Create a signing key array with a length of 32 bytes and copy the secret into it.
Generate random bytes array of length equal to entropy.
Generate an HMAC signature using the generated random string and the signing key.
Create a new hasher using SHA512/256.
Create a new HMAC instance using the hasher and the provided key.
Write the data into the HMAC.
Sum the HMAC and return the result.
Encode the signature using base64.
Store it in Database for later validation of token.
Concatenate the base64-encoded random string and the encoded signature with a dot.
Return the encoded token, encoded signature, and nil error.
Validate function:
Decode the base64-encoded token and signature.
Generate the HMAC signature using the decoded token and the signing key.
Create a new hasher using SHA512/256.
Create a new HMAC instance using the hasher and the provided key.
Write the data into the HMAC.
Sum the HMAC and return the result.
Encode the signature using base64.
Compare the generated HMAC signature with the encoded signature.
Check out full code here.
PAT Generation with Timestamp (including Token Expiry) PAT generation remains similar to the process described above, with the addition of token expiry for enhanced security.
We will generate a timestamp and append it with the token key with an expiry of 5 minutes. This will be used to check if the token is expired or not. Token expiry is crucial for security reasons, as it limits the window of opportunity for potential attackers to misuse a stolen token.
We will create a base64 encoded string with generated random bytes and timestamp with a ~ sign as a separator. We are adding a separator so it will be easy to find the timestamp at the time of decoding/verification.
By incorporating token expiry into the generation process, we ensure that tokens have a limited lifespan, reducing the risk of unauthorized access if a token is compromised.
This addition enhances the security of our token-based authentication system, complementing the cryptographic measures already in place.
go timestamp logic // generate 32-byte random string tokenKey, err := RandomBytes(entropy) if err != nil { return &#34;&#34;, &#34;&#34;, err } // Adds the expiration timestamp in token // time.Minute * 5 can be replace by environment variable timestamp := time.Now().Add(time.Minute * 5).Unix() // In token we have separated it with ~ // to detect extract the time tokenContent := fmt.Sprintf(&#34;%s~%s&#34;, b64.EncodeToString(tokenKey), b64.EncodeToString([]byte(strconv.FormatInt(timestamp, 10)))) signature := generateHMAC(ctx, []byte(tokenContent), &amp;signingKey) encodingSignature := b64.EncodeToString(signature) encodedToken := fmt.Sprintf(&#34;%s.%s&#34;, tokenContent, encodingSignature) An example of token with a timestamp: b-vKxoHTHh5ELdIDeGy4wppKcRb6m4LCZJETAUTjyGw~MTI1Nzg5NDMwMA.MS4upZ9Fr-XhcLriQbt7Q0-ZC6HTPWp4kqG5h8xEJDg
In the above example, the ~ separator denotes the timestamp, ensuring easy extraction during token verification. The inclusion of token expiry adds an additional layer of security to our authentication system, safeguarding against potential threats.
By implementing token expiry, we ensure that even if a token is intercepted, its usefulness is limited, mitigating the risk of unauthorized access and enhancing overall system security.
PAT Verification At time of verification we will:
Split the tokenKey with ~ and decode it. Decode random bytes and timestamp. Generate HMAC similar to what we did in generation. Check token expiration using the timestamp from the token and current timestamp. And check if it is less than the current timestamp so it is expired. tokenTimestamp &lt; time.Now().Unix() go timestamp validation contentSplit := strings.Split(tokenKey, &#34;~&#34;) // Extract timestamp from the token content if len(contentSplit) != 2 { return fmt.Errorf(&#34;Invalid token content format&#34;) } decodedTokenKey, err := b64.DecodeString(contentSplit[0]) if err != nil { return err } decodedTimestamp, err := b64.DecodeString(contentSplit[1]) if err != nil { return err } expectedMAC := generateHMAC(ctx, []byte(fmt.Sprintf(&#34;%s~%s&#34;, b64.EncodeToString(decodedTokenKey), b64.EncodeToString(decodedTimestamp))), &amp;signingKey) if !hmac.Equal(expectedMAC, decodedTokenSignature) { return fmt.Errorf(&#34;Token signature mismatch&#34;) } tokenTimestamp, err := strconv.ParseInt(string(decodedTimestamp), 10, 64) if err != nil { return fmt.Errorf(&#34;Invalid token timestamp&#34;) } // Check token expiration // fmt.Println(fmt.Sprintf(&#34;%v, %v&#34;, tokenTimestamp, time.Now().Unix())) if tokenTimestamp &lt; time.Now().Unix() || isRevoked(tokenSignature) { // active = false // c.updateActiveFlag(tokenSignature, 0) return fmt.Errorf(&#34;Token expired&#34;) } Check the full code for PAT with time expiry here.
This research took lot of time, I am happy to share it with you. I&rsquo;d like to extend my gratitude to ORY&rsquo;s OpenSource community and their Fosite2 project, from which I was able to learn about the generation and validation of Opaque Access Tokens.
OAuth 2 Opaque Access Tokens are also generated in same way as described above. You check out ORY&rsquo;s Fosite project for more details for only token part of Fosite here.
HMAC: https://www.okta.com/identity-101/hmac/&#160;&#x21a9;&#xfe0e;&#160;&#x21a9;&#xfe0e;&#160;&#x21a9;&#xfe0e;&#160;&#x21a9;&#xfe0e;&#160;&#x21a9;&#xfe0e;&#160;&#x21a9;&#xfe0e;
Fosite: https://github.com/ory/fosite&#160;&#x21a9;&#xfe0e;
`,url:"/posts/personal-access-token/"},"/posts/create-github-action/":{title:"Automate with Precision: Building Custom GitHub Actions",tags:["github","actions","ci/cd","docker","go"],categories:["GitHub"],keywords:["GitHub Actions","Docker Container Action","Custom GitHub Actions","GitHub Workflow","GitHub Marketplace","YAML Workflow","Dockerfile for Actions","GitHub Automation","Publish GitHub Action","GitHub Action Testing"],content:`After writing my Go Dash blog, I got an idea of whether I could push the same blog to Medium. I started looking for ways to automate this thing. I checked on the GitHub Marketplace for actions that can push content to Medium directly. I found the GitHub action hugo-to-medium by Prakhar Kaushik.
This GitHub action was good, but there were some issues. In Hugo, I mostly use shortcodes to showcase the sample codes, figures, and images. Another one is that it doesn‚Äôt remove any shortcodes. There was no way for me to remove this from the publishing process. So this issue inspired me to create my own GitHub action, which allows to:
Select and replace shortcodes using regex. Remove the frontmatter of YAML, TOML, or JSON formats from the post. Extract the title and tag from the frontmatter. Support both Markdown and Hugo Markdown formats. TL;DR: If you want to start creating a GitHub action without a backstory, head over to the Creating a GitHub Action section.
While I was researching on how to create my own or custom GitHub actions, I started with the GitHub documentation - it was straight-forward; there were a few steps, but there are three ways to create GitHub Actions:
Docker container action:
It packages the entire environment needed for GitHub Actions. They bundle not only your code but also the specific OS, dependencies, tools, and runtime environment. This packaging approach ensures consistency and reliability because the consumers of your action don&rsquo;t need to worry about installing the necessary tools or dependencies themselves. It tends to be slower than JavaScript action due to the time it takes to build and retrieve the container. It can only execute on a Linux based operating system. If you&rsquo;re using self-hosted runners1, they must also be running a Linux based operating system and have Docker installed to execute Docker container actions. JavaScript action:
It runs directly on the runner1 machine. These actions separate your action&rsquo;s code from the environment used to run that code. It ensure compatibility with all GitHub-hosted runners1 (including Ubuntu, Windows, and macOS). It uses pure JavaScript and existing runner binaries. GitHub Actions Toolkit offers Node.js packages for faster development. Composite action:
Composite actions combine multiple workflow steps into one action. Say you have several run commands that you frequently use together in your workflows. With composite actions, you can bundle these commands into a single, well-defined action. It simplifies workflows by creating reusable actions. Using this composite action in your workflow makes your workflow configuration cleaner and more maintainable. It is great for organizing complex workflows efficiently. Before we start, let&rsquo;s check what mistakes I have made so you can avoid these mistakes.
Mistakes were made Be clear about what you want to achieve. When I read the documentation, I was not clear about which action type I should use. I thought composite action would be the perfect option for me. Let me explain why.
I am thinking it will be straight forward that:
I will write code or logic in Go Build the program binary with the Linux AMD64 option. At the time of release, add it to the Assets of GitHub Releases. When action runs, it will fetch a binary from assets. Using that binary, I will pass the required arguments. The program will handle everything and post content to Medium. But when I have written everything for actions, The program is working as expected and passing unit test cases. I thought now I had to create action.yml with composite action and take all inputs and pass them to the binary. I am happy and excited to test it üòÅ.
As expected, it is not going to run on the first try. I got some errors; actions were not able to fetch the binary (I wrote a script for that). To solve this issue, I removed the script and placed the Go action step, which will directly install Go in the system. Then I can build a binary and execute it. It sounds simple, so the actions are as follows:
yaml action.yml name: &#34;Markdown Or Hugo To Medium&#34; description: &#34;Push hugo markdown post to medium&#34; # declaring input variables for workflow inputs: markdownOrHugo: description: &#34;Specify is it Markdown or Hugo Markdown&#34; required: true default: markdown shortcodes: description: &#34;Shortcodes JSON config file location&#34; required: false replaceHyperlinkToLink: description: &#34;Replace hyperlink to link for medium cards&#34; required: false default: false frontmatterFormat: description: &#34;Select frontmatter format [yaml, toml]&#34; required: false default: &#34;yaml&#34; draft: description: &#34;Publish as a draft on Medium&#34; required: false default: false # running steps runs: using: &#34;composite&#34; steps: - name: Install jq run: sudo apt-get update &amp;&amp; sudo apt-get install -y jq shell: bash - name: Check out the repository to the runner uses: actions/checkout@v4 - name: Setup Go 1.21.x uses: actions/setup-go@v4 with: go-version: &#39;1.21.x&#39; - name: Install dependencies and Build run: | cd \${{ github.action_path }} &amp;&amp; \\ go get . &amp;&amp; \\ go build -o HugoToMedium main.go &amp;&amp; \\ ./HugoToMedium \\ -markdown-or-hugo=\${{ inputs.markdownOrHugo }} \\ -shortcodes-config-file=\${{ inputs.shortcodes }} \\ -replace-hyperlink-to-link=\${{ inputs.replaceHyperlinkToLink }} \\ -frontmatter=\${{ inputs.frontmatterFormat }} \\ -draft=\${{ inputs.draft }} shell: bash # it will be used in GitHub Marketplace next to action name branding: icon: &#34;book-open&#34; color: &#34;blue&#34; This time, I thought everything looked good. But I was getting this error: FATA[2023-09-29T11:17:24Z] repository does not exist - it was generated from binary; it was not able to find the .git directory. I was using it to take the latest commit message.
The conclusion for me was that I was not going to use the composite actions. Now I have two options:
JavaScript actions Docker actions If I use JavaScript actions, I will have to write my logic again in JavaScript, which I have written in Go. So I decided to use Docker Action, which was easy to implement.
Just need to write Dockerfile and update action.yml.
What I learn from this mistake is that I will have to do proper planning next time ü§û.
Let&rsquo;s check out how to create custom GitHub actions. In this, I am going to create Docker actions, but action.yml will have an almost similar syntax.
Creating a GitHub Action As we are creating Docker actions, we need to write a Dockerfile. In your project root directory, create a new Dockerfile file. Make sure that your filename is capitalized correctly. D should be capitalized, as shown above. We will be writing it for my markdown-or-hugo-to-medium.
docker Dockerfile FROM golang:1.21.1-alpine3.18 RUN apk add git COPY . /home/src WORKDIR /home/src RUN GOOS=linux GOARCH=amd64 go build -o HugoToMedium main.go RUN chmod &#43;x HugoToMedium ENTRYPOINT [ &#34;/home/src/HugoToMedium&#34; ] What is going on in the above Dockerfile:
We are pulling golang:1.21.1 alpine image Installing git because if a program is required Copying all the required local files into the /home/src folder of the container Changing Working Directory In this golang:1.21.1 alpine image, golang is already installed. We will use that to build out a binary for the Linux OS with the AMD64 architecture. Update the file permissions to execute. After building the image, when it runs, it should directly run the binary, so we have added the ENTRYPOINT for that. If you want to know more about the Dockerfile instructions for GitHub Action, check out this Document.
Creating Action File Create a new action.yml file in the root directory of your project. This action file will contain what inputs we should get, what the output should be, and what it should run
There are 7 basic parameters:
name* : It will be used to display the name in the Actions tab. author: Name of the action&rsquo;s author. description* : A short description of the action. input: It allows you to specify the data that the action expects to use during runtime. input.&lt;input_id&gt;* : A string identifier to associate with the input. It should be a unique identifier. description* : A string description of the input parameter. required: A boolean to indicate whether the action requires the input parameter. default: A string representing the default value. deprecationMessage: If the input parameter is used, this string is logged as a warning message. You can use this warning to notify users that the input is deprecated and mention any alternatives. output: It allows you to declare data that an action sets. Actions that run later in a workflow can use the output data set of previously run actions. runs*: It specifies whether this is a JavaScript action, a composite action, or a Docker container action and how the action is executed. This is just an overview of the parameters; what are they? For more information, you can check out these parameters in detail on GitHub Docs.
Below is a sample action.yml for Docker actions:
yaml action.yml name: &#34;Markdown Or Hugo To Medium&#34; description: &#34;Push hugo markdown post to medium&#34; # input variables for data that the action expects at runtime inputs: markdownOrHugo: description: &#34;Specify is it Markdown or Hugo Markdown&#34; required: true default: markdown shortcodes: description: &#34;Shortcodes JSON config file location&#34; required: false replaceHyperlinkToLink: description: &#34;Replace hyperlink to link for medium cards&#34; required: false default: false frontmatterFormat: description: &#34;Select frontmatter format [yaml, toml]&#34; required: false default: &#34;yaml&#34; draft: description: &#34;Publish as a draft on Medium&#34; required: false default: false # configuration to run action runs: using: &#34;docker&#34; image: &#34;Dockerfile&#34; # passing above inputs as argument to program args: - -markdown-or-hugo=\${{ inputs.markdownOrHugo }} - -shortcodes-config-file=\${{ inputs.shortcodes }} - -replace-hyperlink-to-link=\${{ inputs.replaceHyperlinkToLink }} - -frontmatter=\${{ inputs.frontmatterFormat }} - -draft=\${{ inputs.draft }} branding: icon: &#34;book-open&#34; color: &#34;blue&#34; In the above action.yml file, I have specified name which will be used to show in the Actions tab. On the next line, description what it does. I am taking five inputs, each of which starts with a variable name where the input will be stored. For instance, markdown-or-hugo has its own description of what the variable is or, along with whether this variable should be required or optional, and also sets the default value to it. The same is done for the other four inputs.
In the runs section with using parameter, we specify which type of actions we are using docker, and we also have to configure which image to use. If you specify Dockerfile as a value, it will build a docker image and then use it. You can also directly use public Docker registry containers by specifying docker://image-name:tag. If your program takes arguments, you can pass them with the args parameter, and if you want to pass the above-taken input you can use \${{ inputs.input_variable_name }}.
I don&rsquo;t want to store anything for the output variable. So, I haven&rsquo;t used the output parameter, but if you want to show something you can do like this:
yaml outputs: output_parameter: description: &#39;This will show the output which will be set from container&#39; Note that if we want to use this approach, we need to update the Dockerfile, and Entrypoint will be a shell script. From that shell script, we can set the output variable.
bash entrypoint.sh #!/bin/sh # Run your Go program and set GITHUB_OUTPUT program_input=$(your_go_program) # Set the output variable echo &#34;output_parameter=$program_input&#34; &gt;&gt; $GITHUB_OUTPUT You can check out more about the output here.
branding uses icons and colors to create a badge to personalize and distinguish your action. Badges are shown next to your action name in the GitHub Marketplace. You can find the icon here.
Here you have written your first custom github action.
Testing the GitHub Action Before we publish our GitHub Action, we need to test it. It is strange that there is no way to test this GitHub action locally. I think GitHub should have provided something from which we can easily test this.
At the time of testing, I faced some problems and wasn&rsquo;t able to understand the testing document properly. So I took the wrong way, published the action, and then tested it. Please avoid these mistakes.
There are two ways to test it:
If your repository is public, you can test it by assigning uses with &lt;username&gt;/&lt;repo-name&gt;@&lt;branch-name&gt; in steps. With this method, you can test actions in other repositories as well. This workflow YAML should be located at .github/workflows/filename.yml. yaml .github/workflows/publish-medium.yml on: [push] jobs: hello_world_job: runs-on: ubuntu-latest name: A job to say hello steps: - name: test action step # uses an action from given repo uses: imrushi/markdown-or-hugo-to-medium@main with: markdown-or-hugo: &#39;hugo&#39; shortcodes: &#34;./shortcodes.json&#34; replaceHyperlinkToLink: false frontmatterFormat: &#34;yaml&#34; draft: true Second way is to test it in the current repository, you can use ./ in uses. ./ syntax to use an action available in the same repository. This option will work on both public and private repositories. yaml .github/workflows/publish-medium.yml on: [push] jobs: hello_world_job: runs-on: ubuntu-latest name: A job to say hello steps: - name: test action step # Uses an action in the root directory uses: ./ with: markdown-or-hugo: &#39;hugo&#39; shortcodes: &#34;./shortcodes.json&#34; replaceHyperlinkToLink: false frontmatterFormat: &#34;yaml&#34; draft: true Publishing the GitHub Action As soon as our action has been tested, it&rsquo;s time to publish it. Let&rsquo;s see how we can do that.
To publish GitHub Action to the GitHub Marketplace, your action repository should be public. Follow the below steps to make it public:
First, create a git tag with the version as per semver. git tag -a v1.0.0 -m &quot;release message&quot;
Push the tag to the GitHub repo. git push origin v1.0.0
On the GitHub repository, go to Releases -&gt; Create/Draft a new release.
Mark with ‚úîÔ∏è Publish this Action to the GitHub Marketplace. It also shows if the required things are done or not. Select the primary category and another category. Choose a tag -&gt; select v1.0.0 Give Release title -&gt; v1.0.0 Provide Release Notes. If you want to provide any assets, drag and drop in the box. If your action is not production-ready, mark ‚úîÔ∏è Set as a pre-release. ‚úîÔ∏è Set as the latest release If everything looks good, Hit the Publish release button. Congratulations! üéâ Your action is now available on the GitHub Marketplace!
How to Use the GitHub Action Start by creating a workflow in the .github/workflows directory of your repository (or create the directory if it doesn&rsquo;t exist). Create a YAML file (e.g., main.yml) to define your workflow. Here&rsquo;s an example of a workflow that uses the &ldquo;Markdown Or Hugo To Medium&rdquo; action yaml main.yml on: push: branches: - main jobs: publish-to-medium: runs-on: ubuntu-latest env: POST_DIR: &#34;content/posts&#34; ACCESS_TOKEN: \${{ secrets.MEDIUM_ACCESS_TOKEN }} steps: - name: Checkout Code uses: actions/checkout@v2 # using published GitHub Actions - name: Markdown Or Hugo To Medium uses: imrushi/markdown-or-hugo-to-medium@v1.0.0 # providing inputs to the actions with: markdownOrHugo: &#34;hugo&#34; shortcodes: &#34;./shortcodes.json&#34; replaceHyperlinkToLink: false frontmatterFormat: &#34;yaml&#34; draft: true This workflow is configured to run when changes are pushed to the main branch. It checks out your code, and then it uses the &ldquo;Markdown Or Hugo To Medium&rdquo; action, passing the necessary inputs as specified in the with section.
Commit this workflow YAML file to your repository. Your workflow will now be automatically triggered when you push changes to the main branch. That&rsquo;s it! You&rsquo;ve successfully created, published, and used your custom GitHub Action.
Conclusion In this blog post, we saw how to create a GitHub action using Docker containers. It covers how to prepare and use a Dockerfile and action.yml for GitHub actions.
This blog is about my experience, the mistakes I have made while creating my first GitHub action. I will be creating more GitHub actions for my use cases. You can check them out on my GitHub profile here.
I hope this blog will help you in making your first GitHub action. If you have any questions, please let me know in the comment section. If you find anything new in this blog, share it with others, and if you find any mistakes in it, please let me know and help me make it better. I will definitely fix it.
Thanks for reading! üòä
&ldquo;Runner&rdquo; refers to a virtual machine or container environment where your GitHub Action workflows are executed.&#160;&#x21a9;&#xfe0e;&#160;&#x21a9;&#xfe0e;&#160;&#x21a9;&#xfe0e;
`,url:"/posts/create-github-action/"},"/posts/go-basics-and-a-dash-of-clean-code/":{title:"Go: Basics and a Dash of Clean Code",tags:["go","basics","clean code"],categories:["Go"],keywords:["go","golang","basics of go","clean code","go syntax","go programming for beginners","go programming tips","learning go","go programming","writing clean code in go","history of go","no semicolon","formatting","comments","names","variables","go tips"],content:` If you have read my first blog Let&rsquo;s Go, which is not about the Go programming language, go check it out üòÖ. You can easily tell how much I am obsessed with the Go programming language. After writing my first blog, I have done some thinking, like where to start my Go programming series blog. I have done some research and collected some notes.
As you read the heading of the blog, I will cover some of the basics that I think are important for anyone starting a programming language that also follows some of the principles of clean code. The topics that I have covered in this blog are the effective way you write your Go code and questions that arise in my mind when I have studied Golang, like why there are different types of variable declarations. Why have they created the Go programming language?
Let&rsquo;s find the answer&rsquo;s:
History of Go The Google search engine experienced an issue back in 2007.
Programs containing millions of lines of code needed to be maintained. Before they could experiment with any new code changes, they had to compile the program, which took them about an hour at that time. String processing is Google&rsquo;s additional issue. Numerous text files, or web pages, are read and analyzed by Google. This was obviously not good for the developers because it made them less productive.
So Google engineers Robert Griesemer, Rob Pike, and Ken Thompson sketched out some goals for a new language:
Fast Compilation (Efficiency) Less cumbersome code (Simplicity) Unused memory freed automatically (Garbage Collection) Easy-to-write software that does several operations simultaneously (Concurrency) Good support for processor with multiple cores After a couple years of work, Google had created Go: a language that was fast to write code and produced programs that were fast to compile and run. Google built a rich library of string functions into Go, Garbage Collecting makes strings in Go simple to think about, and efficient in ways some other string libraries are not. The project switched to an open source license in 2009. It‚Äôs now free for anyone to use.
If you&rsquo;re developing a command-line tool in Go, you can generate executable files for Windows, MacOS, and Linux using the same source code. If you&rsquo;re building a web server, Go can assist you in efficiently managing multiple simultaneous user connections. Regardless of your project&rsquo;s nature, Go will aid you in making your code easier to maintain.
Let&rsquo;s start with syntax and clean code principles :
Go file layout Now let‚Äôs look at the code and figure out what it actually means‚Ä¶
go Hello World package main import &#34;fmt&#34; func main() { fmt.Println(&#34;Hello, World&#34;) } Every Go file has three sections:
The package clause Any import statements The actual code Package : A package in Go is a way to organize and structure code into meaningful units, helping with code organization and reusability. It allows control over visibility, promoting encapsulation and dependency management. The main package serves as the entry point for Go programs.
import : Go files almost always have one or more import statements. The import statement is used to bring in external packages that your code relies on. It ensures that your program loads only the necessary packages, making it faster and more efficient than loading everything at once.
actual code : The last part of every Go file is the actual code, which is often split up into one or more functions. A function is a group of code that you call (run) from other places in your program. When a Go program is run, it looks for a function named main and runs that first, which is why we need this function main.
Function : A function is a group of one or more lines of code that you can call (run) from other places in your program.
Below is the code with what it does in comments:
go Hello World // This line says the rest of the code in // this file belongs to the &#34;main&#34; package package main // This says we will be using text-formatting // code from the &#34;fmt&#34; package import &#34;fmt&#34; // The &#34;main&#34; function is special; it gets run // first when your program runs. func main() { // This line displays (&#34;print&#34;) &#34;Hello, World&#34; in // your terminal (or web browser, if you&#39;re using the Go Playground) // // It does this by calling the &#34;Println&#34; function // from the &#34;fmt&#34; package fmt.Println(&#34;Hello, World&#34;) } Note: When a Go program is run, it looks for a function named main and runs that first.
No Semicolons As you can see in our previous program, there are no semicolons to separate statements in Go; we can use semicolons, but it‚Äôs not required (in fact, it‚Äôs generally frowned upon). This design choice was made to enhance code readability and reduce the potential for common programming errors.
Like C, Go&rsquo;s formal grammar uses semicolons to terminate statements, but unlike C, those semicolons do not appear in the source. Instead, the lexer uses a simple rule to insert semicolons automatically as it scans, so the input text is mostly free of them.
How does it know when to add semicolons? The rule is: If a line ends with an identifier (for example: words such as int or float64), a basic value such as a number or a string, or certain specific tokens such as
break continue fallthrough return ++ -- ) }
The Go lexer then adds a semicolon after that token when it encounters a newline, which is \\n.
To put it simply, when there&rsquo;s a chance that a newline could end a statement, Go automatically inserts a semicolon.
One consequence of the semicolon insertion rules is that you cannot put the opening brace of a control structure like if, for, switch, or select on the new line. If you do, a semicolon will be inserted before the brace, which could cause unwanted effects. So write them similar to below:
go if statement if i &lt; f() { g() } not like this:
go bad if statement if i &lt; f() // wrong! { // wrong! g() } Formatting Formatting issues are the most contentious but the least important. People may prefer different formatting styles, so when another developer or person reads the same code, it may take some time for them to grasp if they are not accustomed to the same formatting style. It will be easier if everyone formats their documents the same way.
Go takes an unusual approach and lets the machine take care of most formatting issues. The Go compiler comes with a standard formatting tool called go fmt. This tool reads a program and automatically formats it with consistent indentation, alignment, and comment retention or adjustment to match a standard style.
Next time, whenever you share your code, other Go developers will expect it to be in the standard Go format. With Go, all you have to do is run go fmt.
If you want to try its simple version, head over to the Go playground, write some unformatted code, and hit the format button.
Comments In Go, developers write text annotations within the source code as comments. These annotations are not executed by the program but serve as notes, explanations, or documentation. Comments are essential for providing context, making code more understandable, and documenting code for future reference.
Go provides C-style /* */ block comments and C++-style // line comments.
Single-line comments: Single-line comments start with two slashes // and continue until the end of the line. They are used for adding brief explanations or clarity to a specific line of code. go Single-line comment // This is a single-line comment in Go. var x int // This comment explains the purpose of the variable. Multi-line comments: Multi-line comments are enclosed within /* and */ and can span multiple lines. They are commonly used for documenting larger sections of code, writing package-level documentation, or providing more detailed explanations. go Multi-line comment /* This is a multi-line comment in Go. It can span multiple lines and is useful for providing detailed explanations or comments for larger code blocks. */ Go also has a convention for documenting exported (public) identifiers and packages using special comments, often referred to as &ldquo;comment annotations&rdquo; or &ldquo;comments for the godoc tool&rdquo;. These comments are structured in a way that allows the godoc tool to automatically generate documentation based on them.
For documenting exported identifiers, you can use a comment placed directly before the identifier, starting with the identifier name and a brief description:
go Sample Go Doc Comment // MyFunction is a public function that performs a specific task. func MyFunction() { // Function implementation... } For package-level documentation, you can include a comment at the top of the file:
go Package level doc comment // Package mypackage provides functionality for... package mypackage import &#34;fmt&#34; // ExportedFunction is a function that... func ExportedFunction() { // Function implementation... } To generate documentation from these comments, you can use the godoc command-line tool. Running godoc on your Go code will produce documentation that includes your comments, making it easier for others to understand and use your code. Properly documented code is not only more understandable but also encourages collaboration and code maintenance.
Names Names are as important in Go as in any other language. When coding, we should consider naming variables, functions, arguments, classes, packages, source files, and directories that contain those source files.
According to the book Clean Code by Robert C. Martin, there are some standards for naming:
Choose descriptive and unambiguous names. Make a meaningful distinction. Use pronounceable names. Use searchable names. Replace magic numbers with named constants. Avoid encodings. Don&rsquo;t append prefixes or type information. You can check this points in detail in this blog
Go has one simple set of rules that apply to the names of variables, functions, and types:
A name must begin with a letter and can have any number of additional letters and numbers. The following points determine the visibility of a name (variable, function, and type names) outside a package: If the name of a variable, function, or type begins with a Capital letter, it is considered as Exported and can be accessed from a package outside the current one. Example - As you have seen in the above hello world program. The P in fmt.Println is capitalized: so it can be used from the main package or any other. If the name begins with a Lowercase letter, it is considered Unexported and can only be accessed within the current package. Above are the only rules that are enforced in Go language. But Go Community follows some additional conventions as well:
When naming something in Go, for instance a variable or a function, use CamelCase. This means that if the name has more than one word, start with a lowercase letter for the first word and then capitalize the first letter of each following word without using spaces. For example, topRank and RetryConnection are in CamelCase, which looks like camel humps. If a name&rsquo;s meaning is clear from the context, it&rsquo;s common to use abbreviations such as i for index or max for maximum to keep the code concise and easy to read. MixedCaps The convention in Go is to use MixedCaps or mixedCaps rather than underscores to write multi-word names.
Package Name Good package names make code better. A package‚Äôs name provides context for its contents, making it easier for the developer or user to understand what the package is for and how to use it. The name also helps package maintainers determine what does and does not belong in the package as it evolves. Well-named packages make it easier to find the code you need.
Package Name Guideline To make a Go package easy to use, it&rsquo;s best to give it a short, clear, and meaningful single-word name. Packages typically have lowercase names without under_scores or mixed capital letters. These names are often simple nouns, for example:
time (provides functionality for measuring and displaying time) list (implements a doubly linked list) http (provides HTTP client and server implementations) Below are examples of bad naming styles in Go:
computeServiceClient priority_queue Abbreviate judiciously. Package names may be abbreviated when the abbreviation is familiar to the programmer. Widely used packages often have compressed names:
strconv (string conversion) syscall (system call) fmt (formatted I/O) Note:- If abbreviating a package name makes it ambiguous or unclear, don‚Äôt do it.
Another convention is that the package name is the base name of its source directory; the package in src/encoding/base64 is imported as &quot;encoding/base64&quot; but has the name base64, not encoding_base64 and not encodingBase64.
Another short example is once.Do; once.Do(setup) reads well and would not be improved by writing once.DoOrWaitUntilDone(setup). Long names don&rsquo;t automatically make things more readable. A helpful doc comment can often be more valuable than an extra-long name.
Interface Names By convention, one-method interfaces are named by the method name plus an -er suffix or similar modification to construct an agent noun; Reader, Writer, Formatter, CloseNotifier etc.
Variable Declaration In Go, a variable is a piece of storage containing a value. You can give a variable a name by using a variable declaration. Just use the var keyword, followed by the desired name and the type of values the variable will hold.
Variable declaration syntax:
var name string
var :- A keyword. name :- A variable name that you want to access in your program. string :- Any datatype that the variable will hold data for. (Go-supported datatype) Once you declare a variable, you can assign any value of that type to it with the = sign.
var name string = &quot;Jerry&quot;
You can assign values to multiple variables in the same statement. Just place multiple variable names on the left side of =, and the same number of values on the right side, separated with commas (,).
Syntax for assigning multiple variables at once:
var length, width float64 = 1.2, 2.4
You can assign new values to existing variables, but they need to be values of the same type, for example: when you assign an int variable value to a string type variable. Go‚Äôs static typing ensures you don‚Äôt accidentally assign the wrong kind of value to a variable.
Short Variable Declaration As we saw in the above section, we can declare variables and assign them values on the same line. But if you know what the initial value of a variable is going to be as soon as you declare it, it‚Äôs more typical to use a short variable declaration. Instead of explicitly declaring the type of the variable and later assigning it with =, you do both at once using :=.
Here are our previous examples with short variable declarations :
name := jerry instead of var name string = &quot;Jerry&quot; length, width := 1.2, 2.4 instead of var length, width float64 = 1.2, 2.4 There‚Äôs no need to explicitly declare the variable‚Äôs type; the type of the value assigned to the variable becomes the type of that variable.
Because short variable declarations are so convenient and concise, they‚Äôre used more often than regular declarations. You‚Äôll still see both forms occasionally, though, so it‚Äôs important to be familiar with both.
Thanks!!! In this blog, we&rsquo;ve delved into Go&rsquo;s intriguing history, exploring its origins and essential conventions such as formatting, comments with the godoc tool, and naming guidelines. We also demystified the absence of semicolons in Go and how they are automatically inserted.
If you found this blog helpful or discovered something new, please consider sharing it with your connections who may benefit from a clearer understanding. If you&rsquo;ve spotted any errors or have additional insights, please don&rsquo;t hesitate to leave a comment below. Let&rsquo;s engage in discussions and learn together.
Once again, a big thank you for joining me on this journey. ü•≥
References Head First GO - Jay McGavren Effective Go - Go Doc Clean Code - Robert C. Martin `,url:"/posts/go-basics-and-a-dash-of-clean-code/"},"/posts/lets-go/":{title:"Let's Go",tags:["start","beginning","blog"],categories:[],keywords:["programming","blogs","first blog","non-technincal","golang"],content:`Begin This is the new beginning and also my second blog, which is also a non-technical blog (first blog). I have decided to start writing blogs so that I can share my thoughts and knowledge and improve my writing skills. I will be writing blogs about mostly technical stuff and sometimes non-technical stuff. In technical blogs, I will cover whatever research I have done, and while doing research, I will keep my notes in the Notes section.
Plan and Present As of now, I don‚Äôt have a full plan for what I am going to write, which topics I will cover, or where it will go. But I remember a quote that says to go with the flow and be more concerned with the journey than the destination.
&ldquo;A good traveler has no fixed plans, and is not intent on arriving.&rdquo; - Lao Tzu
To start with, I will be writing blogs about Golang, which I am currently obsessed with.
As said, I am currently obsessed with Golang, like how it internally works and all. I will start with its basics, and as I go along with my research, I will dig down into how Golang works internally. I am more curious to do research on its internal workings and its concurrency. I will try to explain all the things in an easy way (I hope so üò¨).
As I go with blogs, I will create YouTube videos for the same on my channel. I have lots of project ideas that I will create as an example, and if it is an interesting idea or helpful, I will make it open-source, or you guys can start contributing your ideas to the project.
From my contribution, I have remembered how I struggled with starting to contribute to open-source projects. So definitely, I will write a blog about how to get started with open source. Also,share my experience with it. As of now,I am contributing to vscode-extension for ORY organization, which mostly provides solutions in the areas of authentication, authorization, access control, and zero-trust networking with their open-source projects. I will create a new tab where I can keep track of the ideas.
So my plan will be, in short:
Research on the topic Add to Notes Write blog Make YouTube Videos Let&rsquo;s Go Let us begin this journey and learn as we go. The start may not be great, but you have to start to be great.
If you are interested, I created a track 7 years ago called Let&rsquo;s Go
`,url:"/posts/lets-go/"},"/notes/aws/":{title:"AWS",tags:["AWS"],categories:["Cloud"],keywords:["Serverless"],content:`AWS stands of Amazon Web Service. It offer broad range of cloud services.
Traditional vs Serverless Web hosting Traditional Serverless Provision Capacity Runs On-Demand (unlimited capacity Only pay for Code Executions) Scaling (Pay too much or server issues) Scales Automatically (Pay What you need) Update OS &amp; Software Runs on Managed AWS Infrastructure (up-to-date and secure environment) Lots of Overhead for SPA + API Apps Great for SPA + API Apps API Gateway (REST API) API Gateway is fully managed service that allows you to easily create, publish, maintain, monitor, secure API&rsquo;s at any scale. An API, acts as a &ldquo;front door&rdquo; for applications to access data, functionality, or business logic from your backend services like servers, databases or lambda.
Things API Gateway can do:
Create RESTful API&rsquo;s and WebSockets API&rsquo;s Integration with backend services: API Gateway can route API requests to various backend services like AWS Lambda functions, Amazon EC2 instances, or other web apps. Manage Traffic: API Gateway handles tasks like traffic management, throttling to limit requests, and Cross-Origin Resource Sharing (CORS) for secure communication between different domains. Secure API&rsquo;s: API Gateway helps us to secure APIs with features like IAM authorization and access control. Monitor API&rsquo;s: API Gateway provides metrics and logging to help you monitor the health and performance of our API&rsquo;s. How it works Lambda (LOGIC) It&rsquo;s designed to let us run our code without having to worry about provisioning or managing servers yourself.
Serverless: We don&rsquo;t need to set up or manage servers. AWS Lambda takes care of all the underlying infrastructure, so we can focus on writing code. Event-driven: Our code (packaged as Lambda function) executes in response to events. These events can be triggered by various AWS services like S3 file upload, API Gateway requests, or DynamoDB changes. Automatic Scaling: Lambda automatically scales our code up or down to handle the workload. We have to only pay for the compute time for our code. Cost-effective: Since we only pay for what we use, Lambda can be a cost-effective way to run code that doesn&rsquo;t require constant uptime. Supported languages: Java, Go, Node.js, C#, Python, Ruby and PowerShell DynamoDB (DATA) It is fully managed NoSQL database provided by AWS. It is key-value database similar to mongoDB. It does not support SQL queries. Instead, it uses a propritetary API based on JSON. This API is generally not called directly by user developers, but invoked through AWS SDKs.
DynamoDB is primarily a key-value store in the sense that its data model consists of key-value pairs in a schemaless, very large, non relational table of rows (records). It does not support RDBMS methods to join tables through foreign keys. It can also support a document store data model using JSON.
Data Structure:
Keys :- partition key is key (eg. UserID) and we store value against that partion key. eg. { UserID : firstName }. So here UserID is partition key and firstName is attribute. Partition key should be unique. Note: why is it call partition key? DynamoDB stores data in fleet of SSD and it try to store data effectively by partitioning it over SSDs.
Partition key + sort key Instead just using partition key as primary key dynamoDB gives option to use sort key. Using both partition and sort key we can get new primary key. So when we have same key we can use sort key (timestamp) this gives us primary key.
Attributes :- Attributes are the values stored against the partition key(key). Indexes :- Global Secondary Indexes: Some application might need to perform many kinds of queries, using a variety of different attributes as query criteria. We can set 5 of this per table. Cognito (AUTH) S3 (Simple Storage Service) (Web APP) CloudFront (Cache service) Route53 (DNS) `,url:"/notes/aws/"},"/notes/books/":{title:"Books",tags:[],categories:[],keywords:[],content:` Deep Learning for Coders with fastai - Jeremy Howard (Author), Sylvain Gugger (Author)
bruce lee - matthew polly
So Good They Can&rsquo;t Ignore You - Cal Newport
Introduction to Algorithms - Charles E. Leiserson, Charles E. Leiserson, Ronald L. Rivest, Clifford Stein
System Design Interview ‚Äì An insider&rsquo;s guide - Alex Xu
Head First Go - Jay McGavren
OAuth 2 in Action - Justin Richer, Antonio Sanso
The algorithm design manual
How to solve it by computer
The Data science design manual
`,url:"/notes/books/"},"/notes/regex/":{title:"Regex Cheatsheet",tags:["regex","notes"],categories:[],keywords:["regex basics"],content:`We can use regular expression for regular find operations as well as replace.
Symbol Description Sample Text Regex Output Letters \\w Matches any latin letter or digit Sample \\w S a m p l e \\W Matches any non-alphanumeric character Digits \\d Matches any decimal digit Numbers: 1, 2, 3, 100, 200 \\d Numbers: 1, 2, 3, 1 0 0, 2 0 0 \\D Matches any non decimal digit character. Repetition + Matches one or more repetition of symbol (select whole word or digit) Sample Text. \\w+ Sample Text. Repetition * Matches Zero or more occurrences of the symbol 1 , 2, 3, 10, 20, 100, 200 \\d\\d* 1, 2, 3, 10, 20, 100, 200 Set [] Matches set of characters All symbols inside the braces. [abc] will match a,b or c All symbols inside the braces. [a-c] Works similarly to [abc] [^abc] Match any character except a,b or c [^abc] All symbols inside the braces. Special Symbols \\. \\[ \\+ Match symbols that bear special meaning for regex. need to escape with \\ 2+2 + 2+2 Alteration a|b Will match a or b numbers: 1, 2, 3, 100 [a-f]|\\d numbers: 1, 2, 3, 1 0 0 Single Character . Matches any character other than newline This is test. .* This is test. Optional ? Makes the symbol optional he and she s?he he and she Start Of string ^ Matches the start of a string without consuming characters start of string ^\\w+ start of string End Of string $ Matches the end of string without consuming characters end of string \\w+$ end of string Word Boundary \\b Matches boundaries of word. \\b\\w and \\w\\b will match first and last letter in word. Immortal Gods \\b\\w Immortal Gods Immortal Gods \\w\\b Immortal Gods Space Symbol \\s Match any space symbols This is test. \\s This is test. \\S Matches a single character which is not white space. Number of Repetitions {min_num, max_num} Number of repetitions for an expression. I remember, Nick. \\w{3,5} I remem-ber, Nick {n} Exactly n receptions I remember, Nick. \\w{3} I rem-member, Nick. {n,} N or more I remember, Nick. \\w{3,} I remember, Nick. Grouping () Group of the match by putting them in braces. John Smith (\\w+)\\s(\\w+) John Smith Replacing $1 $2 We can use groups for replacing, referencing by numbers: $1 $2 (selecting group 1 and group 2 using $1 $2) John Smith (\\w+)\\s(\\w+) replace with $2 $1 Smith John Backreferenace (Contents In Capture Group) \\1 It will return a string with the content from the first capture group. 1 can be any number as long as it corresponds to a valid capture group. 6362888232275296622 (\\d)\\1 63628882322752966 22 Lookeaheads (?=) abc(?=d) will match abc only if it is followed by d. abcd abc(?=d) abcd `,url:"/notes/regex/"},"/notes/go/":{title:"Go",tags:["golang","notes"],categories:["Go"],keywords:["programming","go","go notes","technical","golang","golang notes","golang basics","golang syntax"],content:` Go is a programming language that focuses on simplicity and speed. It&rsquo;s simpler than other languages, so it&rsquo;s quicker to learn. And it lets you harness the power of today&rsquo;s multicore computer processor, so your programs run faster.
History of Go Back in 2007, the search engine Google had a problem. They had to maintain programs with millions of line of code. Before they could test new changes, they had to compile the code into runnable form, a process which at the time took the better part of an hour. Needless to say, this was bad for developer productivity.
So Google engineers Robert Griesemer, Rob Pike, and Ken Thompson sketched out some goals for a new language:
Fast Compilation Less cumbersome code Unused memory freed automatically (garbage collection) Easy-to-write software that does serval operations simultaneously (concurrency) Good support for processor with multiple cores After a couple years of work, Google had created Go: a language that was fast to write code for and produced programs that were fast to compile and run. The project switched to an open source license in 2009. It&rsquo;s now free for anyone to use.
If you&rsquo;re writing a command-line tool, Go can produce executable files for Windows, MacOS, and Linux, all from the same source code. If you&rsquo;re writing a web server, it can help you handle many users connecting at once. And no matter what you&rsquo;re what you&rsquo;re writing, it will help you ensure that your code is easier to maintain.
Syntax Basics Go Playground The easiest way to try Go is to visit Go Playground in your web browser. It is simple editor where you can enter Go code and run it on their servers. The result is displayed right there in your browser.
Note: Go Playground requires stable internet connection. If you don&rsquo;t, see Install Go on your system.
Let&rsquo;s try out play ground:
Open Go Playground in your browser. There will be hello world program already written. Click the Format button, which will automatically reformat your code according to Go conventions. Click the Run button. You should see &ldquo;Hello, World!&rdquo; displayed at the bottom of the screen.
Congratulations, you&rsquo;ve just run your first Go programü•≥!
Go file layout Now let&rsquo;s look at the code and figure out what it actually means&hellip;
go Hello World package main import &#34;fmt&#34; func main() { fmt.Println(&#34;Hello, World&#34;) } Every Go file has three sections:
The package clause Any import statements The actual code Package : A package is a collection of code that all does similar things, like formatting strings or drawing images. The package clause gives the name of the package that this file&rsquo;s code will become a part of. In this case, we use the special package main, which is required if this code is going to be run directly (usually from the terminal). import : Go files almost always have one or more import statements. Each file needs to import other packages before its code can use the code those other packages contain. Loading all the Go code on your computer at once would result in a big, slow program, so instead you specify only the package you need by importing them. actual code : The last part of every Go file is the actual code, which is often split up into one or more functions. A function is a group of code that you call (run) from other places in your program. When a Go program is run, it looks for a function named main and runs that first, which is why we need this function main. Below is the code with what it does in comments:
go Hello World // This line says the rest of the code in // this file belongs to the &#34;main&#34; package package main // This says we will be using text-formatting // code from the &#34;fmt&#34; package import &#34;fmt&#34; // The &#34;main&#34; function is special; it gets run // first when your program runs. func main() { // This line displays (&#34;print&#34;) &#34;Hello, World&#34; in // your terminal (or web browser, if you&#39;re using the Go Playground) // // It does this by calling the &#34;Println&#34; function // from the &#34;fmt&#34; package fmt.Println(&#34;Hello, World&#34;) } Function : A function is a group of one or more lines of code that you can call (run) from other places in your program. Note: When a Go program is run, it looks for a function named main and runs that first.
No Semicolons As you can see in our program there are no semicolons to separate statements in Go, we can use semicolons but it&rsquo;s not required (in fact, it&rsquo;s generally frowned upon).
Like C, Go&rsquo;s formal grammar uses semicolons to terminate statements, but unlike in C, those semicolons do not appear in the source. Instead the lexer uses a simple rule to insert semicolons automatically as it scans, so the input text is mostly free of them.
If you want to know more how it works you check Go&rsquo;s official doc https://go.dev/doc/effective_go#semicolons
Formatting Formatting issues are the most contentious but the least important. People may prefer different formatting styles, thus when another developer or person reads the same code it may take some time for him to grasp if he is not accustomed to the same formatting style. It will be easier if everyone formats their documents in the same way.
With Go we take an unusual approach and let the machine take care of most formatting issues. The Go compiler comes with a standard formatting tool, called go fmt. The go fmt program reads a Go program and emits the source in a standard style of indentation and vertical alignment, retaining and if necessary reformatting comments.
Next time whenever you share your code, other Go developers will expect it to be in the standard Go format. With Go all you have to do is run go fmt.
If you want to try its simple version, head over to the Go playground, write some buggy or unformatted code, and hit the format button.
Comments Go provides C style /* */ block comments and C++ style // line comments. Most block comments appear as package comments but are useful within an expression or to disable large blocks of code; Otherwise usually line comments are used.
Comments that appear before a top-level declaration, with no intervening newlines, are considered to document the declaration itself. For example: In the above Hello World programme with comments, all comments will be used in Go Documents. These doc comments are the primary documentation for given Go package or command.
For more about doc comments, see Go Doc Comments.
Names Names are as important in Go as in any other language. Go has one simple set of rules that apply to the names of variables, functions, and types:
A name must begin with letter, and can have any number of additional letters and numbers.
The visibility of a name outside a package is determined by below points:
If the name of a variable, function, or type begins with a Capital letter, it is considered Exported and can be accessed from packages outside the current one. Example - As you have seen in above hello world program. The P in fmt.Println is capitalized: so it can be used from the main package or any other. If the name begins with a Lowercase letter, it is considered Unexported and only be accessed within the current package. Above the only rules enforced by the language. But the Go community follows some additional conventions as well:
If a name consists of multiple words, each word after the first should be capitalized, and they should be attached together without spaces between them, like this: topRank, RetryConnection&hellip; This style is often called Camel Case because the capitalized letter look like the humps of a camel.
When the meaning of a name is obvious from the context, the Go community&rsquo;s convention is to abbreviate it: to use i instead of index, max instead of maximum&hellip;
MixedCaps The convention in Go is to use MixedCaps or mixedCaps rather than underscores to write multiword names.
Package Names Good package names make code better. A package&rsquo;s names provides context for its contents, making it easier for developer/user to understand what the package is for and how to use it. The name also helps package maintainers determine what does and does not belong in the package as it evolves. Well-named packages make it easier to find the code you need.
Guideline It&rsquo;s helpful if everyone using the package can the same name to refer to its contents, which implies that the package name should be good: short, concise, evocative. By convention, packages are given lower case, single-word names; there should be no need for under_scores or mixedCaps. They are often simple nouns, such as:
time (provides functionality for measuring and displaying time) list (implements a doubly linked list) http (provides HTTP client and server implementations) Below are example for bad naming styles in Go:
computeServiceClient priority_queue Abbreviate judiciously. Package names may be abbreviated when the abbreviation is familiar to the programmer. Widely-used packages often have compressed names:
strconv (string conversion) syscall (system call) fmt (formatted I/O) Note:- If abbreviating a package name makes it ambiguous or unclear, don‚Äôt do it.
Another convention is that the package name is the base name of its source directory; the package in src/encoding/base64 is imported as &quot;encoding/base64&quot; but has name base64, not encoding_base64 and not encodingBase64.
Another short example is once.Do; once.Do(setup) reads well and would not be improved by writing once.DoOrWaitUntilDone(setup). Long names don&rsquo;t automatically make things more readable. A helpful doc comment can often be more valuable than an extra long name.
Interface Names By convention, one-method interfaces are named by the method name plus and -er suffix or similar modification to construct an agent noun; Reader, Writer, Formatter, CloseNotifier etc.
Declaration Variables In Go, a variable is a piece of storage containing a value. You can give a variable a name by using a variable declaration. Just use the var keyword followed by the desired name and the type of values the variable will hold.
Variable declaration syntax:
var name string
var :- It is a keyword. name :- It will be a variable name that you want to access in your programme. string :- It will be any datatype that the variable will hold data for. (Go-supported datatype) Once you declare a variable, you can assign any value of that type to it with = sign.
var name string = &quot;Jerry&quot;
You can assign values to multiple variables in the same statement. Just place multiple variable names on the left side of =, and the same number of values on the right side, separated with commas (,).
Syntax for assign multiple variables at once:
var length, width float64 = 1.2, 2.4
You can assign new values to existing variables, but they need to be values of the same type like you can&rsquo;t assign int variable value to string type variable. Go‚Äôs static typing ensures you don‚Äôt accidentally assign the wrong kind of value to a variable.
Short Variable Declaration As we seen in the above section we can declare variables and assign them values on the same line. But if you know what the initial value of a variable is going to be as soon as you declare it, it‚Äôs more typical to use a short variable declaration. Instead of explicitly declaring the type of the variable and later assigning to it with =, you do both at once using :=.
Here are our previous examples with short variable declaration :
name := jerry instead of var name string = &quot;Jerry&quot; length, width := 1.2, 2.4 instead of var length, width float64 = 1.2, 2.4 There‚Äôs no need to explicitly declare the variable‚Äôs type; the type of the value assigned to the variable becomes the type of that variable.
Because short variable declarations are so convenient and concise, they‚Äôre used more often than regular declarations. You‚Äôll still see both forms occasionally, though, so it‚Äôs important to be familiar with both.
Pointers Functions A function is a group of statements that together perform a task. Function can be used to:
Reuse code in multiple places. Make code more organized and readable. Hide implementation details. Improve code performance. Functions are declared using the func keyword, followed by the function name, a list of parameters in parentheses (), and a block of code. The function body is enclosed in curly braces ({ and }). A function can take zero or more arguments.
Syntax for function in Go: func funcName(var1 dataType, var2 dataType,... varN dataType) returnType {}
Creating Function and Calling Function Let&rsquo;s create a sample addition program which will contains function with name add() it will take 2 arguments x and y. Which will be int type and return int (Don&rsquo;t worry we will check return and data types next sections.).
go Addition Function package main import &#34;fmt&#34; func add(x, y int) int { return x &#43; y } func main() { fmt.Println(add(15, 10)) } In the above Addition Function program we have two functions. The first is main(), which doesn&rsquo;t take any arguments (arguments are passed inside rounded brackets ()). The second function is our add function, which you can see we have started with the func keyword to declare a function, followed by the function name add(), and we have passed two arguments x and y, which are type of int. When two or more consecutively named function parameters or arguments share a type, you can omit the type from all but the last.
In the above example, we shortened:
x int, y int to x, y int
The function is returning int data type, which is single value, with return statement statement of x + y, which is an addition of numbers.
To call this function, we need to type the function name (add in this case) and a pair of parentheses with arguments separated by a comma (,) in our case, which is 15, 10.
A parameter is a variable, local to a function, whose value is set when the function is called. When the function is run, each parameter will be se to a copy of the value in the corresponding arguments.
If you check the above program Println is also a function. Let&rsquo;s break down the structure of fmt.Println() and see what is happening here.
fmt. :- It is an package which contain multiple function. Println :- Println is function name which resides in fmt package. To use Println package should be imported then only we can access function it offers. () :- By using parentheses we are executing the function. If the function takes a number of arguments and we don‚Äôt pass any or provide too few or too many, it will give you an error message saying how many arguments were expected, and you will need to fix your code.
Function parameters receive copies of the arguments As we mentioned, when you call a function that has parameters declared, you need to provide arguments to the call. The value in each argument is copied to the corresponding parameter variable. It is also called pass-by-value.
Go is a &ldquo;pass-by-value&rdquo; language; function parameters receive a copy of the arguments from the function call.
This is fine in most cases. But if you want to pass a variable&rsquo;s value to a function and have it change the value in some way, you&rsquo;ll run into trouble. The function can only change the copy of the value in it&rsquo;s parameter, not the original. So any changes you make within the function won&rsquo;t be visible outside it!
For example:
Pass-by-value Now, we wanted to move the statement that prints the addition value from the add function back to the function that calls it (in this case main). It won&rsquo;t work, because add function only alters its copy of the value. In the calling function, when we try to print, we&rsquo;ll get the original value, not the addition one!
Pass-by-value Printing outside add function There is a way to allow a function to alter the original value of variable holds, rather than a copy. We do this using pointers which also called as &ldquo;pass-by-reference&rdquo;.
Multiple Return Value One of Go&rsquo;s unusual features is that functions and methods can return multiple values. This feature is quite useful in various situations where you need to return more than one piece of information from a function. Multiple return values allow you to efficiently handle errors, return status code, or return additional context information along with the primary result.
Below is Division program which return multiple values like quotient, remainder :
go Division package main import ( &#34;fmt&#34; ) func divideAndRemainder(dividend, divisor int) (int, int) { quotient := dividend / divisor remainder := dividend % divisor return quotient, remainder } func main() { quotient, remainder := divideAndRemainder(10, 3) fmt.Printf(&#34;Quotient: %d, Remainder: %d\\n&#34;, quotient, remainder) } In the above example, the divideAndRemainder function takes two integer parameters, dividend and divisor. It calculates the quotient and remainder of the division operation and returns both values as tuple (or pair) of integers. In Go, you specify the return types in parentheses immediately after the function signature. In below declaration (int, int) is returning pair of integers in function return value.
func divideAndRemainder(dividend, divisor int) (int, int) {}
When you call the divideAndRemainder function in the main function, you can capture both return values (quotient and remainder) and use them as needed.
Named Result Parameters Named Result Parameters allow us to declare names from the return values of a function in it&rsquo;s signature. Named result parameters are particularly useful for improving the readability and documentation of a code. They make it clear what each return value represents and can be especially helpful in functions with multiple return values.
go Division with Named Result Parameters package main import &#34;fmt&#34; func divideAndRemainder(dividend, divisor int) (quotient int, remainder int) { quotient = dividend / divisor remainder = dividend % divisor return } func main() { q, r := divideAndRemainder(10, 3) fmt.Printf(&#34;Quotient: %d, Remainder: %d\\n&#34;, q, r) } In this example, the divideAndRemainder function has named result parameters quotient and remainder ((quotient int, remainder int)). Inside the function body, you assign values to these variables, and you don&rsquo;t need to use the return statement explicitly. Go will automatically return the values of quotient and remainder when the function exits.
Benefits of using named result parameters:
**_Documentation and clarity_**: It provide self-documentation for the function, making it clear what each return value represents. This can improve code readability and maintainability. **_Simplify return statement_**: You don&rsquo;t need to explicitly list the return values in the return statement. This simplifies the code and reduces redundancy. **_Avoid variable shadowing_**: When you use named result parameters, you can avoid variable shadowing issues that may occur if you redeclare the same variable names in a nested block. **_Facilitate readability in complex function _**: In functions with many return values or complex logic, using named result parameters can make it easier to understand the meaning of each return value. Note:- Named result parameters are implicitly declared as local variables within the function. You can assign values to them directly, and they will be returned when the function exits. However,you cannot use the := short declaration operator to declare and assign values to named result parameters withing the same line; you should use the = assignment operator.
Defer In Go, the defer statement is used to schedule a function call to be executed just before the surrounding function returns. It allows you to ensure that certain cleanup or finalization tasks are performed regardless of how the function exits, whether it&rsquo;s due to normal execution or an error.
How defer statement works in Go:
**_Deferred functions are executed in reverse order_**: When you use defer to schedule a function call, Go adds it to a stack. The deferred functions are executed in reverse order, meaning the last scheduled function will be executed first, and so on. This behavior is useful when you need to reverse some action or cleanup resources. **_Deferred functions capture their arguments at the time of the defer statement_**: If you pass arguments to a deferred function, those arguments are evaluated immediately, and their values are captured at the time of the defer statement, not at the time the function is executed. This can lead to some interesting behavior in cases where the values of variables change before the function executes. A simple example to illustrate how defer works:
go Defer working package main import ( &#34;fmt&#34; ) func main() { defer fmt.Println(&#34;This will be executed last&#34;) defer fmt.Println(&#34;This will be executed second&#34;) fmt.Println(&#34;This will be executed first&#34;) } In this example, when the main function is executed, it first prints &ldquo;This will be executed first,&rdquo; then schedules the two fmt.Println functions using defer. These deferred functions will be executed in reverse order when the main function is about to return.
In practice, you often use defer for resource cleanup, like closing files, releasing locks, or other cleanup tasks, to ensure that these tasks are performed even if there&rsquo;s an early return or an error condition.
Bullet Points When a function returns multiple values, the last value usually has a type of error. Error values have an Error() method that returns a string describing the error. By convention, functions return an error value of nil to indicate there are no errors. You can access the value a pointer holds by putting a * right before it: *myPointer. If a function receives a pointer as a parameter, and it updates the value at that pointer, then the updated value will still be visible outside the function. Methods A function is a standalone piece of code that can be called by other parts of your program.
A method is a function that is associated with a specific type or struct.
The term method came up with object-oriented programming. In an OOP language (like C++ for example) you can define a class which encapsulates data and functions which belongs together. Those functions inside a class are called methods and you need an instance of that class to call such a method.
In Go, the terminology it is basically the same, although Go isn&rsquo;t an OOP language in the classical meaning. A function which takes a receiver is usually called a method (probably just because people are still used to the terminology of OOP).
So, For example:
go Function func MyFunction(a, b int) int { return a &#43; b } // Usage: // MyFunction(1, 2) but
go Method type MyInteger int func (a MyInteger) MyMethod(b int) int { return a &#43; b } // Usage: // var x MyInteger = 1 // x.MyMethod(2) Formatting Verbs General %v the value in a default format %+v when printing structs, adds field names %#v a Go-syntax representation of the value %T a Go-syntax representation of the type of the value %% a literal percent sign; consumes no values Boolean %t the word true or false Integer %b base 2 (binary) %c the character represented by the corresponding Unicode code point %d base 10 (decimal) %o base 8 (octal- uses number between 0 - 7) %O base 8 with 0o prefix %q a single-quoted character literal safely escaped with Go syntax %x base 16 (hexadecimal), with lowercase letters for a-f %X base 16 (hexadecimal), with uppercase letters for A-F %U Unicode format: U+1234; same as &ldquo;U+%04X&rdquo; Floting-point and complex constituents %b decimalless scientific notation with exponent a power of two %e scientific notation, e.g. -1.234456e+78 %E scientific notation, e.g. -1.234456E+78 %f decimal point but exponent, e.g. 123.456 %F synonym for %f %9f width 9, default precision %.2f default width, precision 2 %9.2f width 9, precision 2 %9.f width 9, precision 0 %g %e for large exponents, %f otherwise %G %E for large exponents, %F otherwise %x hexadecimal notation (with decimal power of two exponent), e.g. -0x1.23abcp+20 %X upper-case hexadecimal notation, e.g. -0X1.23ABCP+20 String and slice of bytes %s the uninterpreted bytes of the string or slice %q a double-quoted string safely escaped with Go syntax %x base 16, lower-case, two characters per byte %X base 16, upper-case, two characters per byte Slice %p address of 0th element in base 16 notation, with leading 0x Pointer %p base 16 notation, with leading 0x The default format for %v is bool %t int, int8 etc. %d uint, uint8 etc. %d, %#x if printed with %#v float32, complex64, etc %g string %s chan %p pointer %p flags + always print a sign for numeric values; guarantee ASCII-only output for %q (%+q) - pad with spaces on the right rather than the left (left-justify the field) # alternate format: add leading 0b for binary (%#b), 0 for octal (%#o) &rsquo; &rsquo; (space) leave a space for elided sign in numbers (% d); put spaces between bytes printing strings or slices in hex (% x, % X) 0 pad with leading zeros rather than spaces Data Structures Array An array is a collection of values that all share the same type. Think of it like one of those pill boxes with compartments &ndash; you can store and retrieve pills from each compartment separately, but it&rsquo;s also easy to transport the container as a whole.
The values an array holds are called its elements. You can have an array of string, booleans, or an array of any other Go type (even an array of array). You can store an entire array in a single variable, and then access any element within the array that you need.
An array holds a specific number of elements, and it cannot grow or shrink. To declare a variable that holds an array you need to follow below syntax:
var myArray [4]string
var: Keyword to declare variable. myArray: Variable name which holds array. [4]string: []: In this brackets you specify how much data array should hold. Like in above example [4] it will hold 4 elements. datatype: Which type of data it will store is mentioned here. for example string or int. Elements in an array are numbered, starting with 0. An element&rsquo;s number is called its index.
If you wanted to make an array with the names of people. for example, the first name would be assigned to index 0, the second name would be at 1, and so forth. The index is specified in square brackets.
go Create Array of Names var names [5]string // Create an array of five strings. names[0] = &#34;Goku&#34; // Assign a value to the first element. names[1] = &#34;Vegeta&#34; // Assign a value to the second element. names[2] = &#34;Gohan&#34; // Assign a value to the third element. fmt.Println(names[0]) // Print the first element. fmt.Println(names[1]) // Print the second element. // Output: Goku Vegeta If you doesn&rsquo;t assign a value to names[0] and try to print. It will show empty string lets see why.
Default values in arrays As with variables, when an array is created, all the values it contains are initialized to the zero value for the type that array holds. So an array of int values is filled with zeros by default and same will be for string but instead of zeros it will be empty string.
Zero/default values can make it safe to manipulate an array element even if you haven&rsquo;t explicitly assigned a value to it. For example, here we have an array of integer counters. We can increment any of them without explicitly assigning a value first, because we know they will all start from 0.
go Zero value manipulation var counters [3]int counters[0]&#43;&#43; // Increment the first element from 0 to 1. counters[0]&#43;&#43; // Increment the first element from 1 to 2. counters[1]&#43;&#43; // Increment the third element from 0 to 1. fmt.Println(counters[0], counters[1], counters[2]) 2 0 1 / | \\ / | Has been incremented once / Still at its zero value Has been incremented twice Note:- When an array is created, all the values it contains are initialized to the zero value for the type the array holds.
Okay, but how can assign default values like we do in python and other languages?
Array literals If you know in advance what values an array should hold, you can initialize the array with those values using an array literal. An array literal starts just like an array type, with the number of elements it will hold in square brackets, followed by the type of its elements. This is followed by a list in curly braces of the initial values each element should have. The element values should be separated by commas ,.
[3]int{7, 21, 5}
[3]: Number of elements array will hold. int: Type of elements array will hold. {data, comma, separated} : Comma-separated list of array values. Let&rsquo;s our previous example using array literals, instead of assigning values to the array elements one by one:
var names [4]string = [4]string{&quot;Goku&quot;, &quot;Vegeta&quot;, &quot;Gohan&quot;}
Using an array literal also allows you to do short variable declaration with :=.
names = [4]string{&quot;Goku&quot;, &quot;Vegeta&quot;, &quot;Gohan&quot;}
If you have array string with sentences as value:
text := [3]string{&#34;This is a series of long strings&#34;, &#34;which would be awkward to place&#34;, &#34;together on a single line&#34;} As you see above it will be hard to read if item grow it will be in single line to make it more readable we can break this in multiline as shown below:
text := [3]string{ &#34;This is a series of long strings&#34;, &#34;which would be awkward to place&#34;, &#34;together on a single line&#34;, } But here is catch which you will have to keep in mind when you break it in multiline it should end with , else you will get error/run it problems.
Slices It is a Go data structure that we can add more values to&ndash;it&rsquo;s called slice. A slice is also a list of elements of a particular type, but unlike arrays, tools are available to add or remove elements. Slice don&rsquo;t hold any data themselves. A slice is merely a view into the elements of an underlying array.
To declare the type for a variable that holds a slice, you use an empty pair of square brackets, followed by the type of elements the slice will hold.
var mySlice []string
This is just like the syntax for declaring an array variable, except that you don‚Äôt specify the size.
Unlike with array variables, declaring a slice variable doesn&rsquo;t automatically create a slice. For that, we can call the built-in make function. We pass make the type of the slice we want to create which should be the same as the type of the variable we&rsquo;re going to assign it to, and the length of slice it should create.
// Declare a slice variable var notes []string // Create a slice with seven strings notes = make([]string, 7) Once the slice is created, you assign and retrieve its elements using the same syntax you would for an array.
notes[0] = &#34;go&#34; // Assign a value to the first element notes[1] = &#34;ts&#34; // ... to the second element fmt.Println(notes[0]) // prints the first element fmt.Println(notes[1]) // prints the second element ---Output--- go ts Here, we can prefer a short variable declaration with make. A short variable declaration will infer the variable&rsquo;s type for you.
// Create a slice with five integers, and set up a variable to hold it. numbers := make([]int, 5) numbers[0] = 2 numbers[1] = 3 fmt.Println(numbers[0]) ---Output--- 2 The built-in len function works the same way with slices as it does with arrays. Just pass len a slice, and its length will be returned as an integer. Both for and for&hellip;range loops work just the same with slices as they do with arrays, too.
go For loop letters := []string{&#34;a&#34;,&#34;b&#34;,&#34;c&#34;} // Regular for loop for i := 0; i &lt; len(letters); i&#43;&#43; { fmt.Println(letters[i]) } // For loop using range for _, letter := range letters { fmt.Println(letter) } Slice literals Create slices with initial values directly using slice literals:
Syntax: []type{values} (e.g., []int{1,2,3}) No need for make function. Resembles array literals, but without length in square brackets. go Slice Literals // Assign values using a slice literal. notes := []string{&#34;go&#34;,&#34;bot&#34;, &#34;cool&#34;, &#34;car&#34;, &#34;ninja&#34;, &#34;turtle&#34;} fmt.Println(notes[2], notes[4], notes[5]) // output cool ninja turtle The Slice Operator Every slice is built on top of an underlying array. It&rsquo;s the underlying array that actually holds the slice&rsquo;s data; the slice is merely a view into some (or all) of the array&rsquo;s elements.
When we use the make function or a slice literal to create slice, the underlying array is created for us automatically. We can&rsquo;t access it, expect through the slice. But we can also create a slice from array with the slice operator.
go Array to Slice underlyingArray := [6]string{&#34;go&#34;,&#34;bot&#34;, &#34;cool&#34;, &#34;car&#34;, &#34;ninja&#34;, &#34;turtle&#34;} slice1 := underlyingArray[0:3] fmt.Println(slice1) // Output [go bot cool] In tha above program underlyingArray[0:3] is creating slice using slice operator where it 0 is index of array where the slice should start, and the 3 index of the array that the slice should stop before. In above output we can see that the second index is the index the slice will stop before. That is, the slice should include the elements up to, but not including, the second index. If you use underlyingArray[i:j] as a slice operator, the resulting slice will actually contain the elements underlyingArray[i] through underlyingArray[j-1].
If you want a slice to include the last element of an underlying array, you actually specify a second index that‚Äôs one beyond the end of the array in your slice operator.
go print with last element underlyingArray := [6]string{&#34;go&#34;,&#34;bot&#34;, &#34;cool&#34;, &#34;car&#34;, &#34;ninja&#34;, &#34;turtle&#34;} slice1 := underlyingArray[3:6] fmt.Println(slice1) // Output [car ninja turtle] Make sure you don‚Äôt go any further than that, though, or you‚Äôll get an error:
go Error access underlyingArray := [6]string{&#34;go&#34;,&#34;bot&#34;, &#34;cool&#34;, &#34;car&#34;, &#34;ninja&#34;, &#34;turtle&#34;} slice1 := underlyingArray[3:7] fmt.Println(slice1) // Output invalid argument: index 7 out of bounds [0:7] Underlying arrays Constants Constants are just Constants in Go.
Constants are created at compile time. It can be Character, String, Boolean, or Numeric values. Because of compile-time restriction, the expression define them must be constant expression, evaluated by a compiler. For instance, 1&laquo;3 is a constant expression, while math.Sin(math.Pi/4) is not because the function call to math.Sin needs to happen at run time. -from go.dev/docs/effective_go
Constants cannot be defined using short declarations (:=). In Go, enumerated constants are created using the iota enumerator.
When and where to use iota Embedding Not Embedding, just the declaration of two struct types working together:
go not embedding type car struct { name string model string } type magazine struct { company car // NOT Embedding level string } This is embedding:
go embedding type car struct { name string model string } type magazine struct { car // Value Semantic Embedding level string } Embed a type using pointer semantics
go pointer semantic embedding type car struct { name string model string } type magazine struct { *car // Pointer Semantic Embedding level string } In this case, a pointer of the type is embedded. In either case, accessing the embedded value is done through the use of the type&rsquo;s name.
The best way to think about embedding is to view the car type as an inner type and magazine as an outer type. It&rsquo;s this inner/outer type relationship that is magical because with embedding, related to the inner type (both fields and methods) can be promoted up to the outer type.
go Outer/inner type promotion package main import &#34;fmt&#34; type car struct { name string model string } type magazine struct { *car // Pointer Semantic Embedding level string } func (c *car) order(quantity int) { fmt.Printf(&#34;Ordering %d copies of magazine \\&#34;%s\\&#34; (%s).\\n&#34;, quantity, c.name, c.model) } func main() { mz := magazine{ car: &amp;car{name: &#34;Honda Accord&#34;, model: &#34;2024&#34;}, level: &#34;Gold&#34;, } mz.car.order(2) mz.order(3) // Outer type promotion } // Output: // Ordering 2 copies of magazine &#34;Honda Accord&#34; (2024). // Ordering 3 copies of magazine &#34;Honda Accord&#34; (2024). Once I add a method named order for car type and then a small main function, I can see the output is the same whether I call the order method through the inner pointer value directly or through the outer type value. The order method declared for the user type is accessible directly by the magazine type value.
Composition The best way to take advantage of embedding is through the compositional design pattern. The idea is to compose larger types from smaller types and focus on the composition of behavior.
go cloud struct type Cloud struct { Host string Timeout time.Duration } func (*Cloud) Pull(d *Data) error { switch rand.Intn(10) { case 1, 9: return io.EOF case 5: return errors.New(&#34;Error reading data from Cloud&#34;) default: d.Line = &#34;Data&#34; fmt.Println(&#34;In:&#34;, d.Line) return nil } } The Cloud type represents a system that I need to pull data from. The implementation is not important. The method Pull can succeed, fail, or not have any data to pull.
go DB struct type DB struct { Host string Timeout time.Duration } func (*DB) Store(d *Data) error { fmt.Println(&#34;Out:&#34;, d.Line) return nil } The DB type also represents a system that I need to store data into. The method Store can succeed or fail.
Note:- Above both methods implementation is not important here. Ignore the implementation.
These two types represent a primitive layer of code that provides the base behavior required to solve the business problem of pulling data out of Cloud and storing that data into DB.
go Pull and Store func Pull(c *Cloud, data []Data) (int, error) { for i := range data { if err := c.Pull(&amp;data[i]); err != nil { return i, err } } return len(data), nil } func Store(d *DB, data []Data) (int, error) { for i := range data { if err := d.Store(&amp;data[i]); err != nil { return i, err } } return len(data), nil } These two functions, Pull and Store are build on the primitive layer of code by accepting a collection of data values to pull or store in the respective systems. These functions focus on the concrete types of Cloud and DB since those are the systems the program needs to work with at this time.
go Copy func func Copy(sys *System, batch int) error { data := make([]Data, batch) for { i, err := Pull(&amp;sys.Cloud, data) if i &gt; 0 { if _, err := Store(&amp;sys.DB, data[:i]); err != nil { return err } } if err != nil { return err } } } The Copy function builds on top of the Pull and Store functions to move all the data that is pending for each run. If I notice the first parameter to Copy, it&rsquo;s a type called System.
go System Struct type System struct { Cloud DB } The System type is to compose a system that knows how to Pull and Store. In this case, composing the ability to Pull and Store from Cloud and DB.
go Main func main() { sys := System{ Cloud: Cloud{ Host: &#34;localhost:8000&#34;, Timeout: time.Second, }, DB: DB{ Host: &#34;localhost:9000&#34;, Timeout: time.Second, }, } if err := Copy(&amp;sys, 3); err != io.EOF { fmt.Println(err) } } The main function can be written to construct a Cloud and DB within th composition of a System. Then the System can be passed to the Copy function and data can begin to flow between the two systems.
Now I have our first draft of a concrete solution to a concrete problem.
Decoupling With Interfaces Methods A function is called a method when that function has a receiver declared. The receiver is the parameter that is declared between the keyword func and the function name. There are two types of receivers, value receivers for implementing value semantics and pointer receivers for implementing pointer semantics.
go method type user struct { name string email string } func (u user) notify() { fmt.Printf(&#34;Sending User Email To %s&lt;%s&gt;\\n&#34;, u.name, u.email) } func (u *user) changeEmail(email string) { u.email = email fmt.Printf(&#34;Changed user email to %s\\n&#34;, email) } The notify function is implemented with a value receiver. This means the method operates under value semantics and will operate on its own copy of the value used to make the call. (It will get the copy of value so if we update it will not affect outside method.)
the changeEmail function is implemented with a pointer receiver. This means the method operates under pointer semantics and will operate on shared access to the value used to make the call. (It will update the value in struct.)
Interfaces Interfaces give programs structure and encourage design by composition. They enable and enforce clean division between components. The standardization of interfaces can set clear and consistent expectation.
Decoupling means reducing the dependencies between components and the types they use. This leads to correctness, quality and maintainability.
Interface allow me to groupe concrete data together by what the data can do. It&rsquo;s about focusing on what data can do and not what the data is. Interface also help my code decouple itself from change by asking concrete data based on what it can do. It&rsquo;s not limited to one type of data.
Interfaces should describe behavior and not state. They should be verbs and not nouns.
Use an interface when:
Users of the API need to provide an implementation detail. API&rsquo;s have multiple implementations they need to maintain internally. Parts of the API that can change have been identified and require decoupling. Don&rsquo;t use an interface:
For the sake of using an interface. To generalize an algorithm. When users can declare their own interfaces. If it&rsquo;s not clear how the interface makes the code better. `,url:"/notes/go/"},"/notes/imp-links/":{title:"Imp Links",tags:["notes"],categories:[],keywords:["links"],content:" Name Links Regex http://regextutorials.com/ ",url:"/notes/imp-links/"},"/notes/system-design/":{title:"System Design",tags:["system design","notes"],categories:["System Design"],keywords:["programming","system design","database","scaling"],content:` Zero To Millions of Users In this section, we will explores the process of scaling a system from supporting a single user to eventually serving millions of users.
Single Server Setup In any system development journey, it&rsquo;s best to begin with a simple step, and that applies even to complex systems. We initiate this process by running everything on a single server. This single server setup includes web services, applications, databases, caching, and more.
Single Web Server Working User access websites through domain names, such as mysite.dev. Domain Name System (DNS) is a third-party paid service not hosted by our system. DNS translates the domain name into an IP address, such as 10.43.23.18 in the figure. Once the IP address is obtained, HTTP requests are sent directly to the web server. The web server returns HTML pages or JSON response for rendering. The traffic to this single server comes from two primary sources:
Web App: It utilizes server-side languages (e.g., Java, Go, Python) for business logic and storage, along with client-side languages like HTML and JavaScript for presentation. Mobile App: HTTP protocol facilitates communication between the mobile app and the web server. JSON is commonly used for API responses due to its simplicity. Scaling with Multiple Servers As the user base grows, relying on a single server is no longer sufficient. We need to move to a multi-server setup, separating web/mobile traffic from the database. This separation allows for independent scaling of web/mobile traffic servers and database servers.
Single Web Server with DB Choosing the Right Database When it comes to databases, there are two main categories to consider: Relational Database and Non-Relational Database.
Relational Databases:
These are often referred to as relational database management system (RDBMS) or SQL database. Data is structured in tables and rows. SQL is used for joining data from different tables. Popular databases options: MySQL, PostgresSQL, Oracle database, MSSQL, etc. This technology has been around for more than 40 years. If relational databases doesn&rsquo;t suit your specific use case, it&rsquo;s essential to explore beyond relational databases.
Non-Relational Databases:
Also known as NoSQL databases. Categorized into: Key-value stores Graph stores Column stores Document stores Typically, non-relational databases do not support join operations. Popular databases options: CouchDB, Neo4j, Cassandra, MongoDB, Amazon DynamoDB, etc. Non-relational databases might be the right choice if: Requires super-low latency. Deal with unstructured data. Need to store a massive amount of data. Scaling the Database There are two ways to scale database: Vertical Scaling and Horizontal Scaling
Vertical Scaling:
This involves adding more power (CPU, RAM, etc.) to your existing servers. Vertical scaling is suitable when traffic is low, and its simplicity is an advantage. However, it has limitations, including a hard limit on resources and a lack of failover and redundancy. Horizontal Scaling:
This approach involves adding more servers to your resource pool. It is more suitable for large-scale applications, addressing the limitations of vertical scaling. In the previous design, users connected directly to the web server, leading to potential issues like server unavailability or slow responses during high traffic. To address these challenges, load balancing comes into play.
Load Balancer Load balancers distribute incoming traffic evenly among a set of web servers defined in a load-balanced configuration.
Single Web Server Working As shown in above diagram:
Users connect to the public IP of the load balancer directly. The load balancer then directs the request to one of the web servers via a private IP for enhanced security. Private IPs are used for server-to-server communication within the same network. This setup resolves the issue of server unavailability and enhances web tier availability. For instance:
If one server goes offline, traffic is automatically redirected to another server, preventing downtime. As website traffic grows, additional servers can be added to the pool, and the load balancer will distribute requests accordingly. The current design includes a single database, which lacks failover and redundancy. To address this, let&rsquo;s explore database replication.
Database Replication Database replication involves establishing a master-slave relationship between the master and the slave databases.
The master database primarily handles write operations. Slave databases replicate data from the master but are typically reserved for read operations. Any data-modifying commands, such as insertions, deletions, or updates, are directed to the master database. Most application requires a much higher ratio of reads to writes; thus, the number of slave db in system usually larger than the number of master db. Database Replication Advantages of database replication:
Enhanced Performance: In a master-slave model, the master handles write and update operations, while read tasks are efficiently distributed to slave nodes, improving query processing performance. Reliability: Data is preserved, even in the event of natural disasters, preventing data loss. We do not need to worry about data loss because data is replicated across multiple locations. High availability: Even if one server breaks, the website continues to function, using data from another server, ensuring smooth operation. What if one of the databases goes offline? If there&rsquo;s only one slave database available, and it experiences an outage, read operations will temporarily shift to the master database. As soon as the issue is detected, a new slave database will replace the faulty one. In cases where multiple slave databases are in operations, read operations are rerouted to other healthy slave databases. A new database server will promptly replace the problematic one. In the event of the master database going offline, one of the slave databases will be promoted to assume the role of the new master. All database operations will be temporarily executed on this newly appointed master database. Simultaneously, a new slave database will be introduced to ensure data replication continues seamlessly. In practical production systems, promoting a new master is more intricate, as the data in a slave database may not be up-to-date. This necessitates the execution of data recovery script to reconcile the missing data. Although alternative replication methods such as multi-master and circular replication exist, they tend to be more intricate in nature.
Let&rsquo;s examine the system design:
When a user wants to access the system, they obtain the IP address of the load balancer via DNS. Using this IP address, the user establishes a connection with the load balancer. The load balancer routes the HTTP request to either Server 1 or Server 2. A web server retrieves user data from slave database. Any data-modifying operations, such as write, update, or delete actions, are directed to the master database. With understanding of the web and data tiers, the next step is to enhance the systems&rsquo;s response time. This can be achieved by introducing a cache layer and transferring static content like JavaScript, CSS, Images, and Video files to a content delivery network (CDN).
Cache A cache is a temporary storage. It stores frequently accessed data in memory so that requests are served more quickly. Whenever web page loads, one or more database call are executed to fetch data. It highly impact the performance of the application by calling the database repeatedly. The cache can mitigate this problem.
Cache tier is a temporary data store layer, much faster that the database. Having a separate cache tier benefits the:
System Performance Reduce Database Workloads Scale the Cache Tire Independently Caching strategy depends on the data and data access patterns.
Below are Caching Strategies:
Read-Through Cache Setup of cache server After receiving a request:
Web server checks if data exists in cache. If exists it returns the data. If not exists, it queries database for data, and save the response data in cache and send it back to client. This caching strategy is called a read-through cache. Pros:
Automated Data Fetching: Read-through caches automate the process of fetching data from database and populating the cache, reducing the burden on application logic and developers. This can simplify application code and improve maintainability. Data Consistency: Since data is fetched and populated by the cache provider, there&rsquo;s a higher likelihood of data consistency between and the database, reducing the risk of serving stale data. Optimal for Read-Heavy Workloads: Read-through caches are particularly well-suited for read-heavy workloads where the same data is requested frequently. They minimize the load on the primary data store(database) and improve response times for commonly accessed data, such as news stories. Cons:
Cache Miss Penalty: The first request for data in a read-through cache always results in a cache miss, incurring the extra penalty of loading data to the cache. This can introduce latency for the initial requests. Warming or pre-heating: To mitigate the cache miss penalty, developers often need to manually warm or pre-heat the cache by issuing queries in advance. This adds complexity and requires careful management. Lack of Flexibility: Unlike Cache-aside, where the data model in the cache can differ from that in the database, read-through caches typically do not allow for different data models. This limitation may be a drawback in some scenarios. Cache-Aside Application directly talks to both the cache and the database. No connection between the cache and the database. All operations to cache and database are handled by the application. Cache Aside Here is what happing:
When the application need data, it first checks the cache for the data. If the data is not found in the cache (a cache miss), the application fetches the data from the primary data store (eg. a database). After fetching the data, the application insert or updates it in the cache, associating it with a specific key. The update can be synchronous or asynchronous, depending on the design. The application uses the data from the cache for subsequent read requests until the data expires or is invalidated. Pros:
Read-Heavy Workloads: Cache-Aside is well-suited for read-heavy workloads, efficiently reducing the load on the primary database. Resilience to Cache Failures: Systems using Cache-Aside remain operational even if the cache cluster fails since they can directly access the database. This provide resilience and ensure system availability. Flexible Data Models: Cache-Aside allows for different data models in the cache compared to the database. It&rsquo;s versatile for storing responses resulting from multiple queries against a request ID. Cons:
Data Inconsistency: When data is written to the database, the cache may become inconsistent. Developers often rely on Time to Live (TTL) to serve stale data, but this can lead to inconsistencies and issues with data freshness. Stale Data: In cases where TTL is used, there&rsquo;s a risk of serving stale data until the expiration, which might not be suitable for applications requiring up-to-date information. Lack of Cache Consistency: Cache-Aside doesn&rsquo;t guarantee cache consistency, potentially resulting in multiple clients fetching and updating the same data simultaneously. This can lead to data inconsistency. Peak Load Issues: If the cache fails during peak loads, response times can get worse(deteriorate), and in extreme cases, it might even overwhelm the database, impacting system performance. Write-Through Cache In this strategy data is written both to the cache and to the underlying data store, such as a database, simultaneously. This ensure that the cache and the data store always have consistent data. When a write operation is performed, it is first written to the cache and then immediately written to the data store. This approach is often used in scenarios where data consistency is critical, but it can introduce higher latency for write operations due to the additional write to the data store. Write-Through Cache When the application performs a write operation(eg., update, insert,etc.), the data is first written to the write-through cache. The Cache immediately forwards the write operation to the underlying database to ensure data consistency. The Cache always reflects the most up-to-date data in the database, maintaining data consistency. Pros:
Data Consistency Guarantee: Ensures strong data consistency without cache invalidation. Seamless Integration with Read-Through Cache: Works well with read-through caching for comprehensive performance and consistency. Simplified Cache Management: Eliminates the need for complex cache invalidation strategies. Ensures that the cache remains synchronized with the data store. Cons:
Write Latency: Introduces extra write latency due to dual write operations. Storage Overhead: May required increased storage, leading to higher infrastructure costs. Write-Around Cache Data is written directly to the data store(database) and only the data that is read makes it way into cache. Pros:
When it combine with read-through it provides good performance in situations where data is written once and read less frequently or never. Write-Back Cache (Write-Behind) In this write operations are initially written to the cache and subsequently asynchronously written to the underlying data store (database). It is used to optimize write operations, reduce write latency, and improve application performance by acknowledging writes quickly, without waiting for the data to be written to the data store (database) immediately. Write-Back (or Behind) Cache Pros:
Reduce Write Latency: Improves application responsiveness by acknowledging writes quickly. Optimized Write Throughput: Allows the application to continue processing without write delay. Improved Application Performance: Allows the application to continue processing without write delays. Mitigation of Write Spikes: Smooth out the load on the data store during write bursts. Cons:
Data Inconsistency Risk: Potential data inconsistency between the cache and the data store. Data Loss Risk: Risk of data loss in case of cache or system failures. Complex Cache Management: Requires additional mechanisms for data consistency. Storage and Infrastructure Costs: Increase operational costs due to added storage and resources. Determine the appropriate times to implement caching. Opt for caching when data is regularly read but seldom updated. Since cached data resides in volatile memory, it&rsquo;s not suitable for data persistence. For example, if a cache server undergoes a restart, all in-memory data is erased. Consequently, it&rsquo;s crucial to store critical data in durable data repositories.
Reference: https://codeahoy.com/2017/08/11/caching-strategies-and-how-to-choose-the-right-one/
CDN (Content Delivery Network) A Content Delivery Network (CDN) is geographically distributed network of proxy servers and their data centers that provide high availability and performance by distributing the service to end-users.
CDNs cache content like web pages, images, and video (static contents) in proxy servers near to the physical location of the user, allowing them to access internet content from a web-enabled device or browser more quickly through a server near them.
Benefits of CDNs:
It reduces latency in communication created by a network design. It improves website performance. It support core network infrastructure, such as: Reducing page load time Reducing Bandwidth costs Increasing protection against security attacks and downtime. CDN Workflow User A access the image.png through application. In application embedded image.png url is CDN url. (request will go to nearest CDN server for client.) If image.png is not in CDN server, it requests to origin server. Origin server stores content in CDN Cache. It returns the loaded content (image.png) to User A. User B is accesses same image.png. CDN returns the content faster than User A. As Content is already present in CDN server. The Content will be present in CDN cache as long as the TTL has not expired.
`,url:"/notes/system-design/"},"/notes/oauth2/":{title:"OAuth2 and OpenID Connect",tags:[],categories:[],keywords:[],content:`OAuth 1.0 In 2006, when several web services companies, including Twitter and Ma.Gnolia, had complementary applications and wanted their users to be able to connect them together. At the time, this type of connection was typically accomplished by asking the user for their credentials on the remote system and sending those credentials to the API. These websites let people log in using a shared login system called OpenID, so they didn&rsquo;t need to create separate accounts for each site. As a consequence, there were no username or passwords that could be used for the API.
To overcome this, the developers sought to create a protocol that would allow their users to delegate access to the API. They based their new protocol on several proprietary implementation of this same concept, including Google&rsquo;s AuthSub and Yahoo!&rsquo;s BBAuth. In all of these, a client application is authorized by a user and receives a token that can then be used to access a remote API. These tokens were all issued with a public and private portion, and this protocol used a novel cryptographic singing mechanism so that it could be used over non-TLS HTTP connections. They called their protocol OAuth 1.0 and published it as an open standard on the web.
OAuth 1.0 was a monolithic protocol designed to provide one mechanism to solve all use cases, and it was venturing into uncomfortable territory. Soon after the publication of RFC 5849, the Web Resource Access Protocol (WRAP) was published. WRAP did away with many of OAuth 1.0&rsquo;s more confusing and problem-prone aspects, such as its custom signature calculation mechanism. After much debate in the community, WRAP was decided on as the basis for the new OAuth 2.0 protocol. Where OAuth 1.0 was monolithic, OAuth 2.0 was modular. The modularity in OAuth 2.0 allowed it to be a framework that could be deployed and used in all of the ways that OAuth 1.0 had been in practice, but without twisting core aspects of the protocol.
In 2012, the core OAuth 2.0 specifications were ratified by the IETF, but the community was far from done with it. This modularity was further codified by splitting the specification into two complementary pieces:
RFC 6749 :- details how to get a token RFC 6750 :- details how to use a particular type of token (the Bearer token) at a protected resource. The core of RFC 6749 details multiple ways to get a token and provides an extension mechanism. Instead of defining one complex method to fit different deployment models, OAuth 2.0 defines four different grant types, each suited to a different application type.
OAuth 2.0 OAuth 2.0 is an open authorization protocol, which enables application to access each other&rsquo;s data. OAuth 2.0 does not share password data but instead uses authorization tokens to prove an identity between consumers and service providers. In simple term it uses authorization process to jump from one service to another without tapping in a new username and password. If you&rsquo;re logged into Google and used those credentials for any app (Sign with Google), you&rsquo;ve used OAuth.
OAuth2 is the industry-standard protocol that enables secure machine-to-machine communication and grants limited access to data and services on behalf of users. The specification also covers delegated access to client types such as browser-based, server-side web, native/mobile apps, and connected devices.
OAuth was originally designed for applications to get access to API&rsquo;s (All they need to ability to access the API). OAuth doesn&rsquo;t communicate with user information. OpenID communicate with user information.
What exactly OAuth 2.0 do? According to the specification that defines it:
The OAuth 2.0 authorization framework enables a third-party application to obtain limited access to an HTTP service, either on behalf of a resource owner by orchestrating an approval interaction between the resource owner an the HTTP service, or by allowing the third-party application to obtain access on its own behalf.
OAuth is all about getting the right of access from one component of a system to another. In the OAuth world, a client application wants to gain access to a protected resources on behalf of a resource owner (usually an end user).
Components of OAuth:
Resource Owner - It has access to an API and can delegate access to that API. The resource owner is usually a person and is generally assumed to have access to a web browser.
Protected Resource - This is the component that the resource owner has access to. The communication can be of any form, but for the most part it&rsquo;s a web API of some kind. Even the name &ldquo;resource&rdquo; makes it sounds like this is something to be downloaded, these APIs can allow read, write, and other operations just as well.
Client - This is the piece of software that accesses the protected resources on behalf of the resource owner. The name &ldquo;client&rdquo; might make you think this is the web browser, but that&rsquo;s not how the term is used here. If you take &ldquo;client&rdquo; in business terms you might think of person who&rsquo;s paying for your services, but that&rsquo;s not what it is. In OAuth, the Client is whatever software consumes the API that makes up the protected resource.
Credential Sharing (Credential Theft) One approach, popular in the enterprise space, is to copy the user‚Äôs credentials and replay them on another service. Users uses the same credentials at both the client application (eg. Photo printer) and the protected resource (eg. storage site). When the user logs into the printer, the printer replays the user&rsquo;s username and password at the storage site, mimicking the user&rsquo;s authentication. User authenticates to the client using centrally controlled credentials eg. username/password, domain session cookie. Client replays the obtained credentials to the protected resource, acting as the user. The protected resource assumes direct user authentication, establishing the connection between the client and the resource. This approach requires that the user have the same credentials at the client application and the protected resource, which limits the effectiveness of this credential-theft technique to a single security domain. Successful if the printing and storage services are offered by the same company, and the user has the same account credentials on both services. Can&rsquo;t we do better than this?
Limited credential, issued separately for each client and each user combination to be used at protected resources. Then we can tie limited rights to each of these limited credentials. Network based protocol that allowed the generation and secure distribution of these limited credentials across security boundaries. In a way that&rsquo;s both user-friendly and scalable to the internet as a whole? Delegating access OAuth is a protocol designed to do exactly above thing: in OAuth, the end user delegates some part of their authority to access the protected resources to the client application to act on their behalf.
To make that happen, OAuth introduces another component into the system: Authorization serve
Authorization Server (AS) :- The authorization server is trusted by the protected resources to issue special purpose security credential called OAuth Access Token to clients.
To get the token, the client first sends the resource owner to the authorization server in order to request that the resource owner authorize this client. The resource owner authenticates to the authorization server and presented with a choice of whether to authorize the client making the request. Instead of giving full access to the functionality, or scopes, resource owner can limit access. Once the authorization grant has been made, then client can then request an access token from the authorization server. This access token can be used at the protected resource to access the API, as granted by the resource owner. In this process resource owner&rsquo;s credentials never exposed to the client. The resource owner authenticates to the authorization server separately from anything used to communicate with the client. Client doesn&rsquo;t have any type of high-powered developer key from which it can access the resources instead it must be authorized by a valid resource owner before it can access any protected resources.
OAuth process at a high level User never has to see or deal with the access token directly. Instead of requiring the user to generate tokens and past them into clients, the OAuth protocol make this process easier. It simple for the client to request a token and the user to authorize the client. Clients can then manage the tokens, and user can manage the client application. There are several ways to get an access token using OAuth.
Beyond HTTP Basic and the password-sharing antipattern OAuth 2.0 Actors OAuth 2.0 has different actors defined in the authentication and authorization process. These actors work together to ensure that the user&rsquo;s information is kept secure and that the application only accesses the information that the user has explicitly granted permission for.
OAuth2 Actors `,url:"/notes/oauth2/"},"/notes/":{title:"note",tags:[],categories:[],keywords:[],content:"",url:"/notes/"}}</script><script src=/js/lunr.min.js></script><script src=/js/search.js></script><footer class=footer><div class=footer__inner><a href=/index.xml class=rss-link target=_blank><img src=/img/svgs/rss.svg alt=rss height=30px width=30px></a><div class=copyright><span>¬© 2025 Rushi Panchariya</span>
<span>:: All rights reserved. üöÄ</span></div></div></footer><script type=text/javascript src=/bundle.min.js></script></div></body></html>